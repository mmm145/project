{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Project Check-in 6"],"metadata":{"id":"QYSAVU74Thzy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"40zBVu-IIUpt"},"outputs":[],"source":["# libraries for neural network\n","\n","import torch\n","import torch.nn as nn\n","from  torch.optim import SGD\n","from torch.optim.lr_scheduler import LinearLR\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","from sklearn.metrics import roc_curve, roc_auc_score\n","from sklearn.metrics import precision_recall_curve\n","\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","pd.set_option('display.max_columns', None)\n","pd.options.mode.copy_on_write = True"],"metadata":{"id":"AXChJnD7VFdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using the URL for the file\n","url = 'https://raw.githubusercontent.com/gengon7/CSM148-Data/refs/heads/main/StudentPerformanceFactors.csv'\n","\n","# Read the CSV file from the URL\n","student_performance_factors = pd.read_csv(url)\n","\n","student_performance_factors.head(5)\n","student_performance_factors.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-81TGxafVIoa","executionInfo":{"status":"ok","timestamp":1734005781920,"user_tz":480,"elapsed":292,"user":{"displayName":"Sonal Aggarwal","userId":"09254205623437368739"}},"outputId":"c34b19f9-229e-4d7f-f76d-d0da20b3e4d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6607, 20)"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["The number of observations is 6607, so the batch size would be 16, 32, or 64.\n","This time we will use 32."],"metadata":{"id":"5hv5o2JcVDjk"}},{"cell_type":"code","source":["num_cols = student_performance_factors.select_dtypes(include=np.number).columns\n","cat_cols = student_performance_factors.select_dtypes(exclude=np.number).columns\n","\n","\n","## Since a very small percentage of values are missing for these variables, we would not lose a significant amount of data by dropping the rows with missing values\n","miss_cat_cols = [\"Teacher_Quality\",\"Parental_Education_Level\",\"Distance_from_Home\"]\n","student_performance_factors = student_performance_factors.dropna(subset=miss_cat_cols)"],"metadata":{"id":"YIM8hyY2cvQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encoding\n","Ordinal_cols = [\"Parental_Involvement\",\"Access_to_Resources\",\"Motivation_Level\",\"Family_Income\",\"Teacher_Quality\",\"Distance_from_Home\", \"Peer_Influence\", \"Parental_Education_Level\"]\n","ord_enc = OrdinalEncoder(categories=[\n","    [\"Low\", \"Medium\", \"High\"],\n","    [\"Low\", \"Medium\", \"High\"],\n","    [\"Low\", \"Medium\", \"High\"],\n","    [\"Low\", \"Medium\", \"High\"],\n","    [\"Low\", \"Medium\", \"High\"],\n","    [\"Near\", \"Moderate\", \"Far\"],\n","    [\"Negative\", \"Neutral\", \"Positive\"],\n","    [\"High School\", \"College\", \"Postgraduate\"]\n","])\n","\n","student_performance_factors[Ordinal_cols] = ord_enc.fit_transform(student_performance_factors[Ordinal_cols])\n","\n","binary_cols = [\"Extracurricular_Activities\",\"Internet_Access\",\"Learning_Disabilities\",\"Gender\",\"School_Type\"]\n","\n","for col in binary_cols:\n","    le = LabelEncoder()\n","    student_performance_factors[col] = le.fit_transform(student_performance_factors[col])\n","\n","spf = student_performance_factors\n","\n","student_performance_factors.head(5)\n","\n","spf.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"yfZO_EpmcyAy","executionInfo":{"status":"ok","timestamp":1734005785411,"user_tz":480,"elapsed":156,"user":{"displayName":"Sonal Aggarwal","userId":"09254205623437368739"}},"outputId":"4af1252a-65cc-4d4b-f6b3-1c18fb45c037"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Hours_Studied  Attendance  Parental_Involvement  Access_to_Resources  \\\n","0             23          84                   0.0                  2.0   \n","1             19          64                   0.0                  1.0   \n","2             24          98                   1.0                  1.0   \n","3             29          89                   0.0                  1.0   \n","4             19          92                   1.0                  1.0   \n","\n","   Extracurricular_Activities  Sleep_Hours  Previous_Scores  Motivation_Level  \\\n","0                           0            7               73               0.0   \n","1                           0            8               59               0.0   \n","2                           1            7               91               1.0   \n","3                           1            8               98               1.0   \n","4                           1            6               65               1.0   \n","\n","   Internet_Access  Tutoring_Sessions  Family_Income  Teacher_Quality  \\\n","0                1                  0            0.0              1.0   \n","1                1                  2            1.0              1.0   \n","2                1                  2            1.0              1.0   \n","3                1                  1            1.0              1.0   \n","4                1                  3            1.0              2.0   \n","\n","   School_Type  Peer_Influence  Physical_Activity  Learning_Disabilities  \\\n","0            1             2.0                  3                      0   \n","1            1             0.0                  4                      0   \n","2            1             1.0                  4                      0   \n","3            1             0.0                  4                      0   \n","4            1             1.0                  4                      0   \n","\n","   Parental_Education_Level  Distance_from_Home  Gender  Exam_Score  \n","0                       0.0                 0.0       1          67  \n","1                       1.0                 1.0       0          61  \n","2                       2.0                 0.0       1          74  \n","3                       0.0                 1.0       1          71  \n","4                       1.0                 0.0       0          70  "],"text/html":["\n","  <div id=\"df-0c6bceca-9331-487c-b62c-5f5634c72a05\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Hours_Studied</th>\n","      <th>Attendance</th>\n","      <th>Parental_Involvement</th>\n","      <th>Access_to_Resources</th>\n","      <th>Extracurricular_Activities</th>\n","      <th>Sleep_Hours</th>\n","      <th>Previous_Scores</th>\n","      <th>Motivation_Level</th>\n","      <th>Internet_Access</th>\n","      <th>Tutoring_Sessions</th>\n","      <th>Family_Income</th>\n","      <th>Teacher_Quality</th>\n","      <th>School_Type</th>\n","      <th>Peer_Influence</th>\n","      <th>Physical_Activity</th>\n","      <th>Learning_Disabilities</th>\n","      <th>Parental_Education_Level</th>\n","      <th>Distance_from_Home</th>\n","      <th>Gender</th>\n","      <th>Exam_Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>23</td>\n","      <td>84</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>73</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>67</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>19</td>\n","      <td>64</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>59</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>61</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>24</td>\n","      <td>98</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>91</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>74</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29</td>\n","      <td>89</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>98</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>71</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>19</td>\n","      <td>92</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>65</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>70</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c6bceca-9331-487c-b62c-5f5634c72a05')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0c6bceca-9331-487c-b62c-5f5634c72a05 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0c6bceca-9331-487c-b62c-5f5634c72a05');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e7b28afc-ffa8-44b9-942d-a47fccbb826d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7b28afc-ffa8-44b9-942d-a47fccbb826d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e7b28afc-ffa8-44b9-942d-a47fccbb826d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"spf","summary":"{\n  \"name\": \"spf\",\n  \"rows\": 6378,\n  \"fields\": [\n    {\n      \"column\": \"Hours_Studied\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 44,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          27,\n          20,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Attendance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 60,\n        \"max\": 100,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          72,\n          60,\n          80\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parental_Involvement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6950202313095585,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Access_to_Resources\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6986353068236706,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Extracurricular_Activities\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sleep_Hours\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 4,\n        \"max\": 10,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          7,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Previous_Scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 50,\n        \"max\": 100,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          64,\n          62\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Motivation_Level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6957582659993654,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Internet_Access\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tutoring_Sessions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          7,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Family_Income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7435262213413567,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Teacher_Quality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6010642687297733,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"School_Type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Peer_Influence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7563941226463406,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Physical_Activity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Learning_Disabilities\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parental_Education_Level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7805695140245485,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Distance_from_Home\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6704415525761296,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Exam_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 55,\n        \"max\": 101,\n        \"num_unique_values\": 45,\n        \"samples\": [\n          96,\n          84\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# response variable is Exam_score\n","\n","# we will implement prediction with neural network.\n","\n","y = student_performance_factors[\"Exam_Score\"]\n","#x = student_performance_factors.drop(columns=[\"Exam_Score\"])\n","x = student_performance_factors[['Hours_Studied', 'Attendance', 'Parental_Involvement',\n","       'Access_to_Resources', 'Previous_Scores', 'Tutoring_Sessions',\n","       'Family_Income', 'Peer_Influence', 'Parental_Education_Level']]\n","\n","y = student_performance_factors[\"Exam_Score\"].values.reshape(-1, 1)\n","# when you implement neural network, you want to scale features because when\n","# updating the weight, we use some type of gradient descent, which depends on the scale of the features.\n","# the larger the feature became, the more contribution the feature have on change by gradient descent, which\n","# leads to the difference among convergence of each weight.\n","\n","scaler_x = StandardScaler()\n","scaler_y = StandardScaler()\n","\n","x = scaler_x.fit_transform(x)\n","y = scaler_y.fit_transform(y).flatten()\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"],"metadata":{"id":"J48bia7tX9Qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","input_size = x_train.shape[1]\n","output_size = 1\n","\n","model = nn.Sequential(\n","    nn.Linear(input_size, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 32),\n","    nn.ReLU(),\n","    nn.Linear(32, output_size)\n",")\n","\n","#0.13804\n","optimizer = SGD(model.parameters(),lr = 0.00055)\n","\n","\n","epochs = 100\n","b_size =32\n","steps_per_epoch = len(x_train) // b_size\n","\n","total_iters = len(x_train) // b_size * epochs\n","\n","for epoch in range(epochs):\n","    idx = np.random.permutation(len(x_train))\n","    totalLoss = 0\n","    for step in range(steps_per_epoch):\n","        start_idx = step * b_size\n","        end_idx = start_idx + b_size\n","\n","        x_batch = x_train[idx[start_idx:end_idx]]\n","        idxx = idx[start_idx:end_idx]\n","\n","        y_batch = y_train[idx[start_idx:end_idx]]\n","        x_batch_tensor = torch.tensor(x_batch, dtype=torch.float32)\n","        y_batch_tensor = torch.tensor(y_batch, dtype=torch.float32).unsqueeze(1)  # Ensure shape (batch_size, 1)\n","\n","        pred = model(x_batch_tensor)\n","        loss = nn.MSELoss()(pred, y_batch_tensor)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        totalLoss += loss.item()\n","\n","        if (step + 1) % 10 == 0:\n","            print(f\"Epoch {epoch + 1}, Step {step + 1}, Loss: {loss.item():.4f}\")\n","    print(f\"Average Loss in Epoch {epoch + 1}: {totalLoss/steps_per_epoch}\")\n","    print(\"-------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Ftfp9U-n8ir","executionInfo":{"status":"ok","timestamp":1734006056973,"user_tz":480,"elapsed":18458,"user":{"displayName":"Sonal Aggarwal","userId":"09254205623437368739"}},"outputId":"7e41a54e-ae58-4830-df1a-0393ac00c184","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Step 10, Loss: 0.6681\n","Epoch 1, Step 20, Loss: 2.0903\n","Epoch 1, Step 30, Loss: 1.1062\n","Epoch 1, Step 40, Loss: 1.0356\n","Epoch 1, Step 50, Loss: 1.2638\n","Epoch 1, Step 60, Loss: 1.1153\n","Epoch 1, Step 70, Loss: 0.6907\n","Epoch 1, Step 80, Loss: 0.6512\n","Epoch 1, Step 90, Loss: 0.5956\n","Epoch 1, Step 100, Loss: 0.9652\n","Epoch 1, Step 110, Loss: 1.5486\n","Epoch 1, Step 120, Loss: 0.6472\n","Epoch 1, Step 130, Loss: 0.7670\n","Epoch 1, Step 140, Loss: 0.5851\n","Epoch 1, Step 150, Loss: 0.9305\n","Average Loss in Epoch 1: 1.0026878883253854\n","-------------------------------------------------\n","Epoch 2, Step 10, Loss: 0.5904\n","Epoch 2, Step 20, Loss: 0.4295\n","Epoch 2, Step 30, Loss: 0.5812\n","Epoch 2, Step 40, Loss: 0.7682\n","Epoch 2, Step 50, Loss: 0.7648\n","Epoch 2, Step 60, Loss: 1.0783\n","Epoch 2, Step 70, Loss: 1.2544\n","Epoch 2, Step 80, Loss: 0.8453\n","Epoch 2, Step 90, Loss: 1.6380\n","Epoch 2, Step 100, Loss: 0.5814\n","Epoch 2, Step 110, Loss: 0.8625\n","Epoch 2, Step 120, Loss: 2.3027\n","Epoch 2, Step 130, Loss: 1.7016\n","Epoch 2, Step 140, Loss: 2.2205\n","Epoch 2, Step 150, Loss: 0.4918\n","Average Loss in Epoch 2: 0.967959647845922\n","-------------------------------------------------\n","Epoch 3, Step 10, Loss: 0.5799\n","Epoch 3, Step 20, Loss: 0.7884\n","Epoch 3, Step 30, Loss: 0.5224\n","Epoch 3, Step 40, Loss: 1.5599\n","Epoch 3, Step 50, Loss: 0.9540\n","Epoch 3, Step 60, Loss: 1.2381\n","Epoch 3, Step 70, Loss: 1.0860\n","Epoch 3, Step 80, Loss: 0.7805\n","Epoch 3, Step 90, Loss: 0.7330\n","Epoch 3, Step 100, Loss: 0.6778\n","Epoch 3, Step 110, Loss: 0.3469\n","Epoch 3, Step 120, Loss: 0.5039\n","Epoch 3, Step 130, Loss: 0.7745\n","Epoch 3, Step 140, Loss: 0.6855\n","Epoch 3, Step 150, Loss: 0.5834\n","Average Loss in Epoch 3: 0.9375680193961041\n","-------------------------------------------------\n","Epoch 4, Step 10, Loss: 0.8156\n","Epoch 4, Step 20, Loss: 0.3609\n","Epoch 4, Step 30, Loss: 0.5628\n","Epoch 4, Step 40, Loss: 0.8866\n","Epoch 4, Step 50, Loss: 2.5307\n","Epoch 4, Step 60, Loss: 0.5676\n","Epoch 4, Step 70, Loss: 0.8511\n","Epoch 4, Step 80, Loss: 2.5903\n","Epoch 4, Step 90, Loss: 0.7071\n","Epoch 4, Step 100, Loss: 0.4752\n","Epoch 4, Step 110, Loss: 0.4717\n","Epoch 4, Step 120, Loss: 0.6927\n","Epoch 4, Step 130, Loss: 0.5718\n","Epoch 4, Step 140, Loss: 0.7106\n","Epoch 4, Step 150, Loss: 0.8032\n","Average Loss in Epoch 4: 0.9075179514270159\n","-------------------------------------------------\n","Epoch 5, Step 10, Loss: 0.5452\n","Epoch 5, Step 20, Loss: 2.3770\n","Epoch 5, Step 30, Loss: 0.5337\n","Epoch 5, Step 40, Loss: 0.8009\n","Epoch 5, Step 50, Loss: 0.5799\n","Epoch 5, Step 60, Loss: 0.8350\n","Epoch 5, Step 70, Loss: 0.7083\n","Epoch 5, Step 80, Loss: 0.6411\n","Epoch 5, Step 90, Loss: 0.5924\n","Epoch 5, Step 100, Loss: 0.5040\n","Epoch 5, Step 110, Loss: 0.6692\n","Epoch 5, Step 120, Loss: 0.8847\n","Epoch 5, Step 130, Loss: 0.5399\n","Epoch 5, Step 140, Loss: 0.4108\n","Epoch 5, Step 150, Loss: 0.9098\n","Average Loss in Epoch 5: 0.8747300569366359\n","-------------------------------------------------\n","Epoch 6, Step 10, Loss: 0.3897\n","Epoch 6, Step 20, Loss: 0.6280\n","Epoch 6, Step 30, Loss: 0.7793\n","Epoch 6, Step 40, Loss: 0.6084\n","Epoch 6, Step 50, Loss: 0.7127\n","Epoch 6, Step 60, Loss: 0.5882\n","Epoch 6, Step 70, Loss: 0.5660\n","Epoch 6, Step 80, Loss: 0.5653\n","Epoch 6, Step 90, Loss: 0.5723\n","Epoch 6, Step 100, Loss: 0.7045\n","Epoch 6, Step 110, Loss: 0.4416\n","Epoch 6, Step 120, Loss: 0.5546\n","Epoch 6, Step 130, Loss: 0.8080\n","Epoch 6, Step 140, Loss: 0.4251\n","Epoch 6, Step 150, Loss: 0.6546\n","Average Loss in Epoch 6: 0.8423249303544842\n","-------------------------------------------------\n","Epoch 7, Step 10, Loss: 0.9233\n","Epoch 7, Step 20, Loss: 2.4554\n","Epoch 7, Step 30, Loss: 0.6746\n","Epoch 7, Step 40, Loss: 1.7738\n","Epoch 7, Step 50, Loss: 0.4887\n","Epoch 7, Step 60, Loss: 0.4773\n","Epoch 7, Step 70, Loss: 0.6645\n","Epoch 7, Step 80, Loss: 0.5526\n","Epoch 7, Step 90, Loss: 0.5116\n","Epoch 7, Step 100, Loss: 2.1856\n","Epoch 7, Step 110, Loss: 0.6827\n","Epoch 7, Step 120, Loss: 0.9323\n","Epoch 7, Step 130, Loss: 0.6357\n","Epoch 7, Step 140, Loss: 0.4805\n","Epoch 7, Step 150, Loss: 0.4994\n","Average Loss in Epoch 7: 0.807214599266742\n","-------------------------------------------------\n","Epoch 8, Step 10, Loss: 0.8330\n","Epoch 8, Step 20, Loss: 0.3649\n","Epoch 8, Step 30, Loss: 1.5258\n","Epoch 8, Step 40, Loss: 0.3807\n","Epoch 8, Step 50, Loss: 0.4795\n","Epoch 8, Step 60, Loss: 1.0935\n","Epoch 8, Step 70, Loss: 1.5433\n","Epoch 8, Step 80, Loss: 0.4249\n","Epoch 8, Step 90, Loss: 1.0757\n","Epoch 8, Step 100, Loss: 0.4372\n","Epoch 8, Step 110, Loss: 0.5753\n","Epoch 8, Step 120, Loss: 0.4059\n","Epoch 8, Step 130, Loss: 0.5920\n","Epoch 8, Step 140, Loss: 0.9446\n","Epoch 8, Step 150, Loss: 0.4984\n","Average Loss in Epoch 8: 0.7688345387109421\n","-------------------------------------------------\n","Epoch 9, Step 10, Loss: 2.1603\n","Epoch 9, Step 20, Loss: 0.4144\n","Epoch 9, Step 30, Loss: 0.6604\n","Epoch 9, Step 40, Loss: 0.3542\n","Epoch 9, Step 50, Loss: 0.3769\n","Epoch 9, Step 60, Loss: 0.3255\n","Epoch 9, Step 70, Loss: 0.4625\n","Epoch 9, Step 80, Loss: 3.9814\n","Epoch 9, Step 90, Loss: 2.3510\n","Epoch 9, Step 100, Loss: 0.2996\n","Epoch 9, Step 110, Loss: 0.3887\n","Epoch 9, Step 120, Loss: 0.3761\n","Epoch 9, Step 130, Loss: 0.5401\n","Epoch 9, Step 140, Loss: 0.5919\n","Epoch 9, Step 150, Loss: 0.2810\n","Average Loss in Epoch 9: 0.7295122920717083\n","-------------------------------------------------\n","Epoch 10, Step 10, Loss: 0.3822\n","Epoch 10, Step 20, Loss: 2.3577\n","Epoch 10, Step 30, Loss: 0.3273\n","Epoch 10, Step 40, Loss: 0.4032\n","Epoch 10, Step 50, Loss: 1.2871\n","Epoch 10, Step 60, Loss: 0.6940\n","Epoch 10, Step 70, Loss: 0.5010\n","Epoch 10, Step 80, Loss: 0.3874\n","Epoch 10, Step 90, Loss: 0.2626\n","Epoch 10, Step 100, Loss: 0.4025\n","Epoch 10, Step 110, Loss: 1.8905\n","Epoch 10, Step 120, Loss: 0.3716\n","Epoch 10, Step 130, Loss: 1.4417\n","Epoch 10, Step 140, Loss: 0.4116\n","Epoch 10, Step 150, Loss: 0.3771\n","Average Loss in Epoch 10: 0.6889041635975148\n","-------------------------------------------------\n","Epoch 11, Step 10, Loss: 0.4152\n","Epoch 11, Step 20, Loss: 0.3300\n","Epoch 11, Step 30, Loss: 0.5104\n","Epoch 11, Step 40, Loss: 0.3872\n","Epoch 11, Step 50, Loss: 2.0321\n","Epoch 11, Step 60, Loss: 0.3900\n","Epoch 11, Step 70, Loss: 0.2106\n","Epoch 11, Step 80, Loss: 0.6443\n","Epoch 11, Step 90, Loss: 0.2838\n","Epoch 11, Step 100, Loss: 0.3401\n","Epoch 11, Step 110, Loss: 1.6416\n","Epoch 11, Step 120, Loss: 3.0043\n","Epoch 11, Step 130, Loss: 0.2391\n","Epoch 11, Step 140, Loss: 1.9447\n","Epoch 11, Step 150, Loss: 0.3836\n","Average Loss in Epoch 11: 0.6469484488919096\n","-------------------------------------------------\n","Epoch 12, Step 10, Loss: 0.3248\n","Epoch 12, Step 20, Loss: 1.0066\n","Epoch 12, Step 30, Loss: 0.2608\n","Epoch 12, Step 40, Loss: 0.2500\n","Epoch 12, Step 50, Loss: 1.7274\n","Epoch 12, Step 60, Loss: 0.2997\n","Epoch 12, Step 70, Loss: 0.3309\n","Epoch 12, Step 80, Loss: 0.3018\n","Epoch 12, Step 90, Loss: 2.0992\n","Epoch 12, Step 100, Loss: 0.3604\n","Epoch 12, Step 110, Loss: 0.3651\n","Epoch 12, Step 120, Loss: 0.3057\n","Epoch 12, Step 130, Loss: 1.1008\n","Epoch 12, Step 140, Loss: 0.4664\n","Epoch 12, Step 150, Loss: 0.2528\n","Average Loss in Epoch 12: 0.6041079833634995\n","-------------------------------------------------\n","Epoch 13, Step 10, Loss: 0.3343\n","Epoch 13, Step 20, Loss: 0.2803\n","Epoch 13, Step 30, Loss: 1.5725\n","Epoch 13, Step 40, Loss: 0.2828\n","Epoch 13, Step 50, Loss: 0.3300\n","Epoch 13, Step 60, Loss: 3.8861\n","Epoch 13, Step 70, Loss: 0.2263\n","Epoch 13, Step 80, Loss: 0.2644\n","Epoch 13, Step 90, Loss: 0.3819\n","Epoch 13, Step 100, Loss: 0.2142\n","Epoch 13, Step 110, Loss: 0.3855\n","Epoch 13, Step 120, Loss: 0.3530\n","Epoch 13, Step 130, Loss: 0.3068\n","Epoch 13, Step 140, Loss: 0.2376\n","Epoch 13, Step 150, Loss: 0.2772\n","Average Loss in Epoch 13: 0.5625935074105952\n","-------------------------------------------------\n","Epoch 14, Step 10, Loss: 0.3551\n","Epoch 14, Step 20, Loss: 0.3487\n","Epoch 14, Step 30, Loss: 0.1498\n","Epoch 14, Step 40, Loss: 0.3090\n","Epoch 14, Step 50, Loss: 1.8847\n","Epoch 14, Step 60, Loss: 0.2358\n","Epoch 14, Step 70, Loss: 0.2409\n","Epoch 14, Step 80, Loss: 0.3293\n","Epoch 14, Step 90, Loss: 0.2310\n","Epoch 14, Step 100, Loss: 0.1534\n","Epoch 14, Step 110, Loss: 0.1181\n","Epoch 14, Step 120, Loss: 1.9893\n","Epoch 14, Step 130, Loss: 1.0536\n","Epoch 14, Step 140, Loss: 0.2098\n","Epoch 14, Step 150, Loss: 0.3114\n","Average Loss in Epoch 14: 0.5151135604711449\n","-------------------------------------------------\n","Epoch 15, Step 10, Loss: 0.1460\n","Epoch 15, Step 20, Loss: 1.0520\n","Epoch 15, Step 30, Loss: 0.3386\n","Epoch 15, Step 40, Loss: 0.2205\n","Epoch 15, Step 50, Loss: 0.2239\n","Epoch 15, Step 60, Loss: 0.2865\n","Epoch 15, Step 70, Loss: 0.3055\n","Epoch 15, Step 80, Loss: 0.2557\n","Epoch 15, Step 90, Loss: 2.2710\n","Epoch 15, Step 100, Loss: 0.2934\n","Epoch 15, Step 110, Loss: 2.1158\n","Epoch 15, Step 120, Loss: 0.8696\n","Epoch 15, Step 130, Loss: 0.2113\n","Epoch 15, Step 140, Loss: 0.2850\n","Epoch 15, Step 150, Loss: 1.5870\n","Average Loss in Epoch 15: 0.493474266204819\n","-------------------------------------------------\n","Epoch 16, Step 10, Loss: 0.2671\n","Epoch 16, Step 20, Loss: 0.2669\n","Epoch 16, Step 30, Loss: 1.3378\n","Epoch 16, Step 40, Loss: 0.0962\n","Epoch 16, Step 50, Loss: 0.1256\n","Epoch 16, Step 60, Loss: 0.2114\n","Epoch 16, Step 70, Loss: 0.1242\n","Epoch 16, Step 80, Loss: 0.1939\n","Epoch 16, Step 90, Loss: 1.7695\n","Epoch 16, Step 100, Loss: 0.2443\n","Epoch 16, Step 110, Loss: 0.1541\n","Epoch 16, Step 120, Loss: 0.1810\n","Epoch 16, Step 130, Loss: 1.3989\n","Epoch 16, Step 140, Loss: 0.2261\n","Epoch 16, Step 150, Loss: 0.2174\n","Average Loss in Epoch 16: 0.451025979277098\n","-------------------------------------------------\n","Epoch 17, Step 10, Loss: 0.1797\n","Epoch 17, Step 20, Loss: 0.1138\n","Epoch 17, Step 30, Loss: 0.8724\n","Epoch 17, Step 40, Loss: 2.8640\n","Epoch 17, Step 50, Loss: 0.1773\n","Epoch 17, Step 60, Loss: 0.7244\n","Epoch 17, Step 70, Loss: 1.3035\n","Epoch 17, Step 80, Loss: 0.6242\n","Epoch 17, Step 90, Loss: 0.2279\n","Epoch 17, Step 100, Loss: 0.1485\n","Epoch 17, Step 110, Loss: 0.1509\n","Epoch 17, Step 120, Loss: 0.0757\n","Epoch 17, Step 130, Loss: 0.2095\n","Epoch 17, Step 140, Loss: 0.1090\n","Epoch 17, Step 150, Loss: 0.1410\n","Average Loss in Epoch 17: 0.4291090396871357\n","-------------------------------------------------\n","Epoch 18, Step 10, Loss: 1.5002\n","Epoch 18, Step 20, Loss: 0.1806\n","Epoch 18, Step 30, Loss: 0.1033\n","Epoch 18, Step 40, Loss: 0.1342\n","Epoch 18, Step 50, Loss: 0.1239\n","Epoch 18, Step 60, Loss: 0.1648\n","Epoch 18, Step 70, Loss: 1.2836\n","Epoch 18, Step 80, Loss: 0.1488\n","Epoch 18, Step 90, Loss: 0.1181\n","Epoch 18, Step 100, Loss: 0.1242\n","Epoch 18, Step 110, Loss: 1.6081\n","Epoch 18, Step 120, Loss: 1.2326\n","Epoch 18, Step 130, Loss: 0.1453\n","Epoch 18, Step 140, Loss: 0.1184\n","Epoch 18, Step 150, Loss: 0.1462\n","Average Loss in Epoch 18: 0.41629469975735406\n","-------------------------------------------------\n","Epoch 19, Step 10, Loss: 0.1155\n","Epoch 19, Step 20, Loss: 0.1039\n","Epoch 19, Step 30, Loss: 0.9621\n","Epoch 19, Step 40, Loss: 0.1388\n","Epoch 19, Step 50, Loss: 0.1787\n","Epoch 19, Step 60, Loss: 0.0969\n","Epoch 19, Step 70, Loss: 0.1119\n","Epoch 19, Step 80, Loss: 0.0970\n","Epoch 19, Step 90, Loss: 1.3984\n","Epoch 19, Step 100, Loss: 0.1492\n","Epoch 19, Step 110, Loss: 0.1337\n","Epoch 19, Step 120, Loss: 0.0991\n","Epoch 19, Step 130, Loss: 1.4841\n","Epoch 19, Step 140, Loss: 0.0830\n","Epoch 19, Step 150, Loss: 0.5481\n","Average Loss in Epoch 19: 0.3995235549204005\n","-------------------------------------------------\n","Epoch 20, Step 10, Loss: 0.1193\n","Epoch 20, Step 20, Loss: 0.1887\n","Epoch 20, Step 30, Loss: 0.1193\n","Epoch 20, Step 40, Loss: 0.1458\n","Epoch 20, Step 50, Loss: 0.1550\n","Epoch 20, Step 60, Loss: 0.1746\n","Epoch 20, Step 70, Loss: 0.1523\n","Epoch 20, Step 80, Loss: 0.1299\n","Epoch 20, Step 90, Loss: 0.0978\n","Epoch 20, Step 100, Loss: 0.1112\n","Epoch 20, Step 110, Loss: 0.0993\n","Epoch 20, Step 120, Loss: 0.1102\n","Epoch 20, Step 130, Loss: 0.1064\n","Epoch 20, Step 140, Loss: 3.1848\n","Epoch 20, Step 150, Loss: 0.0738\n","Average Loss in Epoch 20: 0.37953673518678677\n","-------------------------------------------------\n","Epoch 21, Step 10, Loss: 0.1112\n","Epoch 21, Step 20, Loss: 0.9783\n","Epoch 21, Step 30, Loss: 0.1258\n","Epoch 21, Step 40, Loss: 0.0868\n","Epoch 21, Step 50, Loss: 0.0814\n","Epoch 21, Step 60, Loss: 0.0781\n","Epoch 21, Step 70, Loss: 0.0991\n","Epoch 21, Step 80, Loss: 0.5683\n","Epoch 21, Step 90, Loss: 0.0880\n","Epoch 21, Step 100, Loss: 0.0769\n","Epoch 21, Step 110, Loss: 0.5342\n","Epoch 21, Step 120, Loss: 0.1074\n","Epoch 21, Step 130, Loss: 0.1575\n","Epoch 21, Step 140, Loss: 0.0876\n","Epoch 21, Step 150, Loss: 0.0435\n","Average Loss in Epoch 21: 0.3758620817050244\n","-------------------------------------------------\n","Epoch 22, Step 10, Loss: 0.9115\n","Epoch 22, Step 20, Loss: 0.0831\n","Epoch 22, Step 30, Loss: 0.0927\n","Epoch 22, Step 40, Loss: 0.1162\n","Epoch 22, Step 50, Loss: 0.1287\n","Epoch 22, Step 60, Loss: 0.0759\n","Epoch 22, Step 70, Loss: 1.7959\n","Epoch 22, Step 80, Loss: 0.1204\n","Epoch 22, Step 90, Loss: 0.0951\n","Epoch 22, Step 100, Loss: 0.1106\n","Epoch 22, Step 110, Loss: 0.1209\n","Epoch 22, Step 120, Loss: 0.0618\n","Epoch 22, Step 130, Loss: 0.1215\n","Epoch 22, Step 140, Loss: 0.0707\n","Epoch 22, Step 150, Loss: 0.0971\n","Average Loss in Epoch 22: 0.3678211054571395\n","-------------------------------------------------\n","Epoch 23, Step 10, Loss: 0.0946\n","Epoch 23, Step 20, Loss: 0.0873\n","Epoch 23, Step 30, Loss: 0.7913\n","Epoch 23, Step 40, Loss: 0.0576\n","Epoch 23, Step 50, Loss: 1.4779\n","Epoch 23, Step 60, Loss: 0.0787\n","Epoch 23, Step 70, Loss: 0.1471\n","Epoch 23, Step 80, Loss: 0.0754\n","Epoch 23, Step 90, Loss: 0.0781\n","Epoch 23, Step 100, Loss: 0.0955\n","Epoch 23, Step 110, Loss: 0.0736\n","Epoch 23, Step 120, Loss: 0.1012\n","Epoch 23, Step 130, Loss: 0.0788\n","Epoch 23, Step 140, Loss: 2.4399\n","Epoch 23, Step 150, Loss: 0.0882\n","Average Loss in Epoch 23: 0.3617149623778631\n","-------------------------------------------------\n","Epoch 24, Step 10, Loss: 1.7187\n","Epoch 24, Step 20, Loss: 0.8957\n","Epoch 24, Step 30, Loss: 0.6448\n","Epoch 24, Step 40, Loss: 0.0609\n","Epoch 24, Step 50, Loss: 1.1079\n","Epoch 24, Step 60, Loss: 0.0519\n","Epoch 24, Step 70, Loss: 0.0743\n","Epoch 24, Step 80, Loss: 0.0775\n","Epoch 24, Step 90, Loss: 0.0929\n","Epoch 24, Step 100, Loss: 0.1038\n","Epoch 24, Step 110, Loss: 0.0647\n","Epoch 24, Step 120, Loss: 0.1064\n","Epoch 24, Step 130, Loss: 0.0749\n","Epoch 24, Step 140, Loss: 0.1086\n","Epoch 24, Step 150, Loss: 0.0919\n","Average Loss in Epoch 24: 0.35455495018348004\n","-------------------------------------------------\n","Epoch 25, Step 10, Loss: 0.4977\n","Epoch 25, Step 20, Loss: 1.7360\n","Epoch 25, Step 30, Loss: 2.5147\n","Epoch 25, Step 40, Loss: 0.0760\n","Epoch 25, Step 50, Loss: 0.0520\n","Epoch 25, Step 60, Loss: 0.0904\n","Epoch 25, Step 70, Loss: 0.0618\n","Epoch 25, Step 80, Loss: 1.9858\n","Epoch 25, Step 90, Loss: 0.0697\n","Epoch 25, Step 100, Loss: 0.0776\n","Epoch 25, Step 110, Loss: 0.0519\n","Epoch 25, Step 120, Loss: 0.1058\n","Epoch 25, Step 130, Loss: 0.0761\n","Epoch 25, Step 140, Loss: 0.0542\n","Epoch 25, Step 150, Loss: 0.0687\n","Average Loss in Epoch 25: 0.35323811343537187\n","-------------------------------------------------\n","Epoch 26, Step 10, Loss: 0.0641\n","Epoch 26, Step 20, Loss: 0.0477\n","Epoch 26, Step 30, Loss: 0.1005\n","Epoch 26, Step 40, Loss: 0.0601\n","Epoch 26, Step 50, Loss: 0.0857\n","Epoch 26, Step 60, Loss: 0.0700\n","Epoch 26, Step 70, Loss: 0.1140\n","Epoch 26, Step 80, Loss: 1.3433\n","Epoch 26, Step 90, Loss: 0.0678\n","Epoch 26, Step 100, Loss: 0.0635\n","Epoch 26, Step 110, Loss: 0.0621\n","Epoch 26, Step 120, Loss: 0.0401\n","Epoch 26, Step 130, Loss: 0.0705\n","Epoch 26, Step 140, Loss: 0.0569\n","Epoch 26, Step 150, Loss: 0.0813\n","Average Loss in Epoch 26: 0.3503726060332367\n","-------------------------------------------------\n","Epoch 27, Step 10, Loss: 0.0764\n","Epoch 27, Step 20, Loss: 1.3428\n","Epoch 27, Step 30, Loss: 0.0587\n","Epoch 27, Step 40, Loss: 0.0765\n","Epoch 27, Step 50, Loss: 0.0864\n","Epoch 27, Step 60, Loss: 1.3783\n","Epoch 27, Step 70, Loss: 0.1171\n","Epoch 27, Step 80, Loss: 0.4965\n","Epoch 27, Step 90, Loss: 0.0730\n","Epoch 27, Step 100, Loss: 0.6642\n","Epoch 27, Step 110, Loss: 0.1035\n","Epoch 27, Step 120, Loss: 0.1047\n","Epoch 27, Step 130, Loss: 0.0766\n","Epoch 27, Step 140, Loss: 1.5191\n","Epoch 27, Step 150, Loss: 0.0505\n","Average Loss in Epoch 27: 0.347946767349663\n","-------------------------------------------------\n","Epoch 28, Step 10, Loss: 0.0693\n","Epoch 28, Step 20, Loss: 0.0719\n","Epoch 28, Step 30, Loss: 1.5078\n","Epoch 28, Step 40, Loss: 0.0692\n","Epoch 28, Step 50, Loss: 0.0578\n","Epoch 28, Step 60, Loss: 0.1006\n","Epoch 28, Step 70, Loss: 1.5543\n","Epoch 28, Step 80, Loss: 0.0821\n","Epoch 28, Step 90, Loss: 0.0899\n","Epoch 28, Step 100, Loss: 0.0562\n","Epoch 28, Step 110, Loss: 0.1092\n","Epoch 28, Step 120, Loss: 0.0704\n","Epoch 28, Step 130, Loss: 0.0612\n","Epoch 28, Step 140, Loss: 0.1044\n","Epoch 28, Step 150, Loss: 1.3431\n","Average Loss in Epoch 28: 0.34610650975421164\n","-------------------------------------------------\n","Epoch 29, Step 10, Loss: 0.1000\n","Epoch 29, Step 20, Loss: 0.6134\n","Epoch 29, Step 30, Loss: 0.1202\n","Epoch 29, Step 40, Loss: 0.0404\n","Epoch 29, Step 50, Loss: 0.1010\n","Epoch 29, Step 60, Loss: 0.0796\n","Epoch 29, Step 70, Loss: 0.0974\n","Epoch 29, Step 80, Loss: 0.0598\n","Epoch 29, Step 90, Loss: 2.1280\n","Epoch 29, Step 100, Loss: 0.0802\n","Epoch 29, Step 110, Loss: 1.3848\n","Epoch 29, Step 120, Loss: 0.0555\n","Epoch 29, Step 130, Loss: 0.1024\n","Epoch 29, Step 140, Loss: 0.0788\n","Epoch 29, Step 150, Loss: 1.9182\n","Average Loss in Epoch 29: 0.33435951039757367\n","-------------------------------------------------\n","Epoch 30, Step 10, Loss: 0.0713\n","Epoch 30, Step 20, Loss: 0.0947\n","Epoch 30, Step 30, Loss: 1.7212\n","Epoch 30, Step 40, Loss: 1.8873\n","Epoch 30, Step 50, Loss: 0.0480\n","Epoch 30, Step 60, Loss: 0.0818\n","Epoch 30, Step 70, Loss: 0.0783\n","Epoch 30, Step 80, Loss: 0.0828\n","Epoch 30, Step 90, Loss: 0.6830\n","Epoch 30, Step 100, Loss: 0.0932\n","Epoch 30, Step 110, Loss: 0.0777\n","Epoch 30, Step 120, Loss: 0.0698\n","Epoch 30, Step 130, Loss: 0.8773\n","Epoch 30, Step 140, Loss: 0.0698\n","Epoch 30, Step 150, Loss: 0.0987\n","Average Loss in Epoch 30: 0.34345023237965394\n","-------------------------------------------------\n","Epoch 31, Step 10, Loss: 0.0619\n","Epoch 31, Step 20, Loss: 0.0660\n","Epoch 31, Step 30, Loss: 1.6696\n","Epoch 31, Step 40, Loss: 0.0899\n","Epoch 31, Step 50, Loss: 0.0417\n","Epoch 31, Step 60, Loss: 0.0797\n","Epoch 31, Step 70, Loss: 0.0603\n","Epoch 31, Step 80, Loss: 2.2056\n","Epoch 31, Step 90, Loss: 1.3636\n","Epoch 31, Step 100, Loss: 1.7701\n","Epoch 31, Step 110, Loss: 0.0462\n","Epoch 31, Step 120, Loss: 1.4742\n","Epoch 31, Step 130, Loss: 0.0952\n","Epoch 31, Step 140, Loss: 0.0690\n","Epoch 31, Step 150, Loss: 0.4985\n","Average Loss in Epoch 31: 0.34247369258283816\n","-------------------------------------------------\n","Epoch 32, Step 10, Loss: 2.6929\n","Epoch 32, Step 20, Loss: 0.0653\n","Epoch 32, Step 30, Loss: 0.0618\n","Epoch 32, Step 40, Loss: 0.0601\n","Epoch 32, Step 50, Loss: 0.0596\n","Epoch 32, Step 60, Loss: 0.0756\n","Epoch 32, Step 70, Loss: 0.0489\n","Epoch 32, Step 80, Loss: 0.0636\n","Epoch 32, Step 90, Loss: 1.7029\n","Epoch 32, Step 100, Loss: 0.0795\n","Epoch 32, Step 110, Loss: 2.2937\n","Epoch 32, Step 120, Loss: 0.8989\n","Epoch 32, Step 130, Loss: 0.0753\n","Epoch 32, Step 140, Loss: 0.0655\n","Epoch 32, Step 150, Loss: 0.0569\n","Average Loss in Epoch 32: 0.34132188741328584\n","-------------------------------------------------\n","Epoch 33, Step 10, Loss: 0.0737\n","Epoch 33, Step 20, Loss: 1.8831\n","Epoch 33, Step 30, Loss: 0.0532\n","Epoch 33, Step 40, Loss: 0.0830\n","Epoch 33, Step 50, Loss: 0.0608\n","Epoch 33, Step 60, Loss: 0.0749\n","Epoch 33, Step 70, Loss: 0.0670\n","Epoch 33, Step 80, Loss: 0.0577\n","Epoch 33, Step 90, Loss: 0.6810\n","Epoch 33, Step 100, Loss: 0.0615\n","Epoch 33, Step 110, Loss: 0.0761\n","Epoch 33, Step 120, Loss: 0.0776\n","Epoch 33, Step 130, Loss: 0.0450\n","Epoch 33, Step 140, Loss: 0.0790\n","Epoch 33, Step 150, Loss: 0.0814\n","Average Loss in Epoch 33: 0.3404946062924727\n","-------------------------------------------------\n","Epoch 34, Step 10, Loss: 0.8232\n","Epoch 34, Step 20, Loss: 0.0578\n","Epoch 34, Step 30, Loss: 0.1035\n","Epoch 34, Step 40, Loss: 0.0669\n","Epoch 34, Step 50, Loss: 0.0758\n","Epoch 34, Step 60, Loss: 0.1001\n","Epoch 34, Step 70, Loss: 0.0807\n","Epoch 34, Step 80, Loss: 0.0647\n","Epoch 34, Step 90, Loss: 0.0835\n","Epoch 34, Step 100, Loss: 0.0799\n","Epoch 34, Step 110, Loss: 1.0351\n","Epoch 34, Step 120, Loss: 0.0665\n","Epoch 34, Step 130, Loss: 0.0777\n","Epoch 34, Step 140, Loss: 0.8906\n","Epoch 34, Step 150, Loss: 0.0655\n","Average Loss in Epoch 34: 0.33994573272717826\n","-------------------------------------------------\n","Epoch 35, Step 10, Loss: 0.9403\n","Epoch 35, Step 20, Loss: 0.0884\n","Epoch 35, Step 30, Loss: 0.0477\n","Epoch 35, Step 40, Loss: 0.0709\n","Epoch 35, Step 50, Loss: 0.0729\n","Epoch 35, Step 60, Loss: 0.0619\n","Epoch 35, Step 70, Loss: 0.0457\n","Epoch 35, Step 80, Loss: 0.0698\n","Epoch 35, Step 90, Loss: 0.0608\n","Epoch 35, Step 100, Loss: 0.0719\n","Epoch 35, Step 110, Loss: 0.0688\n","Epoch 35, Step 120, Loss: 0.0663\n","Epoch 35, Step 130, Loss: 0.0492\n","Epoch 35, Step 140, Loss: 0.0615\n","Epoch 35, Step 150, Loss: 0.0639\n","Average Loss in Epoch 35: 0.3310857182544357\n","-------------------------------------------------\n","Epoch 36, Step 10, Loss: 1.3260\n","Epoch 36, Step 20, Loss: 0.0825\n","Epoch 36, Step 30, Loss: 3.0485\n","Epoch 36, Step 40, Loss: 0.0828\n","Epoch 36, Step 50, Loss: 0.6915\n","Epoch 36, Step 60, Loss: 0.0922\n","Epoch 36, Step 70, Loss: 0.0777\n","Epoch 36, Step 80, Loss: 0.0610\n","Epoch 36, Step 90, Loss: 0.0446\n","Epoch 36, Step 100, Loss: 0.0794\n","Epoch 36, Step 110, Loss: 0.0668\n","Epoch 36, Step 120, Loss: 0.0645\n","Epoch 36, Step 130, Loss: 0.0576\n","Epoch 36, Step 140, Loss: 0.1016\n","Epoch 36, Step 150, Loss: 0.0723\n","Average Loss in Epoch 36: 0.338792792410127\n","-------------------------------------------------\n","Epoch 37, Step 10, Loss: 0.5186\n","Epoch 37, Step 20, Loss: 0.0824\n","Epoch 37, Step 30, Loss: 0.0975\n","Epoch 37, Step 40, Loss: 0.1007\n","Epoch 37, Step 50, Loss: 0.4415\n","Epoch 37, Step 60, Loss: 0.0760\n","Epoch 37, Step 70, Loss: 0.0652\n","Epoch 37, Step 80, Loss: 0.0813\n","Epoch 37, Step 90, Loss: 0.0628\n","Epoch 37, Step 100, Loss: 0.0783\n","Epoch 37, Step 110, Loss: 0.0563\n","Epoch 37, Step 120, Loss: 1.0077\n","Epoch 37, Step 130, Loss: 1.3862\n","Epoch 37, Step 140, Loss: 0.0423\n","Epoch 37, Step 150, Loss: 1.3755\n","Average Loss in Epoch 37: 0.3382213312265633\n","-------------------------------------------------\n","Epoch 38, Step 10, Loss: 0.0575\n","Epoch 38, Step 20, Loss: 0.0545\n","Epoch 38, Step 30, Loss: 0.8630\n","Epoch 38, Step 40, Loss: 0.0887\n","Epoch 38, Step 50, Loss: 0.0679\n","Epoch 38, Step 60, Loss: 0.0650\n","Epoch 38, Step 70, Loss: 2.1744\n","Epoch 38, Step 80, Loss: 0.0951\n","Epoch 38, Step 90, Loss: 2.6888\n","Epoch 38, Step 100, Loss: 0.0567\n","Epoch 38, Step 110, Loss: 0.0406\n","Epoch 38, Step 120, Loss: 0.0882\n","Epoch 38, Step 130, Loss: 0.0554\n","Epoch 38, Step 140, Loss: 1.0876\n","Epoch 38, Step 150, Loss: 0.0459\n","Average Loss in Epoch 38: 0.33762650737691224\n","-------------------------------------------------\n","Epoch 39, Step 10, Loss: 0.8740\n","Epoch 39, Step 20, Loss: 0.0576\n","Epoch 39, Step 30, Loss: 0.4967\n","Epoch 39, Step 40, Loss: 0.0569\n","Epoch 39, Step 50, Loss: 0.0733\n","Epoch 39, Step 60, Loss: 0.0526\n","Epoch 39, Step 70, Loss: 0.0680\n","Epoch 39, Step 80, Loss: 0.0606\n","Epoch 39, Step 90, Loss: 0.0738\n","Epoch 39, Step 100, Loss: 0.0573\n","Epoch 39, Step 110, Loss: 1.7890\n","Epoch 39, Step 120, Loss: 0.0767\n","Epoch 39, Step 130, Loss: 3.4290\n","Epoch 39, Step 140, Loss: 0.0862\n","Epoch 39, Step 150, Loss: 1.7059\n","Average Loss in Epoch 39: 0.33728848650770366\n","-------------------------------------------------\n","Epoch 40, Step 10, Loss: 0.0784\n","Epoch 40, Step 20, Loss: 0.0614\n","Epoch 40, Step 30, Loss: 0.0683\n","Epoch 40, Step 40, Loss: 0.6859\n","Epoch 40, Step 50, Loss: 0.0579\n","Epoch 40, Step 60, Loss: 0.0704\n","Epoch 40, Step 70, Loss: 0.0609\n","Epoch 40, Step 80, Loss: 1.5704\n","Epoch 40, Step 90, Loss: 1.0236\n","Epoch 40, Step 100, Loss: 0.0474\n","Epoch 40, Step 110, Loss: 0.0562\n","Epoch 40, Step 120, Loss: 0.8313\n","Epoch 40, Step 130, Loss: 0.0829\n","Epoch 40, Step 140, Loss: 0.1026\n","Epoch 40, Step 150, Loss: 0.0804\n","Average Loss in Epoch 40: 0.3368891693136227\n","-------------------------------------------------\n","Epoch 41, Step 10, Loss: 0.6595\n","Epoch 41, Step 20, Loss: 0.0421\n","Epoch 41, Step 30, Loss: 0.0581\n","Epoch 41, Step 40, Loss: 0.0789\n","Epoch 41, Step 50, Loss: 0.0567\n","Epoch 41, Step 60, Loss: 0.0568\n","Epoch 41, Step 70, Loss: 0.0836\n","Epoch 41, Step 80, Loss: 0.0465\n","Epoch 41, Step 90, Loss: 0.0836\n","Epoch 41, Step 100, Loss: 0.0559\n","Epoch 41, Step 110, Loss: 0.0654\n","Epoch 41, Step 120, Loss: 0.0514\n","Epoch 41, Step 130, Loss: 0.5079\n","Epoch 41, Step 140, Loss: 1.2927\n","Epoch 41, Step 150, Loss: 0.0533\n","Average Loss in Epoch 41: 0.33645730950641184\n","-------------------------------------------------\n","Epoch 42, Step 10, Loss: 0.0866\n","Epoch 42, Step 20, Loss: 0.0725\n","Epoch 42, Step 30, Loss: 0.0485\n","Epoch 42, Step 40, Loss: 0.0667\n","Epoch 42, Step 50, Loss: 0.0714\n","Epoch 42, Step 60, Loss: 0.0576\n","Epoch 42, Step 70, Loss: 0.0681\n","Epoch 42, Step 80, Loss: 1.6753\n","Epoch 42, Step 90, Loss: 0.0637\n","Epoch 42, Step 100, Loss: 0.0723\n","Epoch 42, Step 110, Loss: 1.2185\n","Epoch 42, Step 120, Loss: 0.0838\n","Epoch 42, Step 130, Loss: 0.0566\n","Epoch 42, Step 140, Loss: 0.0680\n","Epoch 42, Step 150, Loss: 0.0556\n","Average Loss in Epoch 42: 0.3359617056243075\n","-------------------------------------------------\n","Epoch 43, Step 10, Loss: 0.0552\n","Epoch 43, Step 20, Loss: 0.0709\n","Epoch 43, Step 30, Loss: 0.0562\n","Epoch 43, Step 40, Loss: 0.0429\n","Epoch 43, Step 50, Loss: 0.0576\n","Epoch 43, Step 60, Loss: 1.3651\n","Epoch 43, Step 70, Loss: 0.0560\n","Epoch 43, Step 80, Loss: 1.2766\n","Epoch 43, Step 90, Loss: 0.1053\n","Epoch 43, Step 100, Loss: 0.0720\n","Epoch 43, Step 110, Loss: 0.0656\n","Epoch 43, Step 120, Loss: 0.0452\n","Epoch 43, Step 130, Loss: 0.0472\n","Epoch 43, Step 140, Loss: 0.0812\n","Epoch 43, Step 150, Loss: 0.0565\n","Average Loss in Epoch 43: 0.33570998971520355\n","-------------------------------------------------\n","Epoch 44, Step 10, Loss: 0.0459\n","Epoch 44, Step 20, Loss: 0.6577\n","Epoch 44, Step 30, Loss: 0.1017\n","Epoch 44, Step 40, Loss: 1.2420\n","Epoch 44, Step 50, Loss: 0.0659\n","Epoch 44, Step 60, Loss: 0.0523\n","Epoch 44, Step 70, Loss: 0.0549\n","Epoch 44, Step 80, Loss: 0.0845\n","Epoch 44, Step 90, Loss: 0.5122\n","Epoch 44, Step 100, Loss: 0.0474\n","Epoch 44, Step 110, Loss: 0.5652\n","Epoch 44, Step 120, Loss: 0.0512\n","Epoch 44, Step 130, Loss: 0.0773\n","Epoch 44, Step 140, Loss: 0.0632\n","Epoch 44, Step 150, Loss: 0.6834\n","Average Loss in Epoch 44: 0.33536874726648974\n","-------------------------------------------------\n","Epoch 45, Step 10, Loss: 0.0987\n","Epoch 45, Step 20, Loss: 0.0535\n","Epoch 45, Step 30, Loss: 0.0763\n","Epoch 45, Step 40, Loss: 0.0322\n","Epoch 45, Step 50, Loss: 0.0550\n","Epoch 45, Step 60, Loss: 0.0657\n","Epoch 45, Step 70, Loss: 0.0373\n","Epoch 45, Step 80, Loss: 0.0964\n","Epoch 45, Step 90, Loss: 0.0425\n","Epoch 45, Step 100, Loss: 0.0441\n","Epoch 45, Step 110, Loss: 1.5372\n","Epoch 45, Step 120, Loss: 0.0576\n","Epoch 45, Step 130, Loss: 0.0620\n","Epoch 45, Step 140, Loss: 0.0591\n","Epoch 45, Step 150, Loss: 0.0879\n","Average Loss in Epoch 45: 0.33500910555041813\n","-------------------------------------------------\n","Epoch 46, Step 10, Loss: 0.0528\n","Epoch 46, Step 20, Loss: 0.0772\n","Epoch 46, Step 30, Loss: 1.4983\n","Epoch 46, Step 40, Loss: 1.8367\n","Epoch 46, Step 50, Loss: 1.8773\n","Epoch 46, Step 60, Loss: 1.3675\n","Epoch 46, Step 70, Loss: 0.0704\n","Epoch 46, Step 80, Loss: 0.0645\n","Epoch 46, Step 90, Loss: 0.0728\n","Epoch 46, Step 100, Loss: 0.0571\n","Epoch 46, Step 110, Loss: 0.0399\n","Epoch 46, Step 120, Loss: 1.7074\n","Epoch 46, Step 130, Loss: 1.2810\n","Epoch 46, Step 140, Loss: 0.0728\n","Epoch 46, Step 150, Loss: 0.4890\n","Average Loss in Epoch 46: 0.33464533238197275\n","-------------------------------------------------\n","Epoch 47, Step 10, Loss: 1.2092\n","Epoch 47, Step 20, Loss: 0.8374\n","Epoch 47, Step 30, Loss: 0.4862\n","Epoch 47, Step 40, Loss: 1.3191\n","Epoch 47, Step 50, Loss: 0.6845\n","Epoch 47, Step 60, Loss: 0.0556\n","Epoch 47, Step 70, Loss: 0.0550\n","Epoch 47, Step 80, Loss: 0.0423\n","Epoch 47, Step 90, Loss: 0.0779\n","Epoch 47, Step 100, Loss: 0.0605\n","Epoch 47, Step 110, Loss: 0.0466\n","Epoch 47, Step 120, Loss: 0.0461\n","Epoch 47, Step 130, Loss: 3.0905\n","Epoch 47, Step 140, Loss: 1.4398\n","Epoch 47, Step 150, Loss: 0.6823\n","Average Loss in Epoch 47: 0.33137233765010937\n","-------------------------------------------------\n","Epoch 48, Step 10, Loss: 1.3402\n","Epoch 48, Step 20, Loss: 0.4640\n","Epoch 48, Step 30, Loss: 0.0781\n","Epoch 48, Step 40, Loss: 0.0551\n","Epoch 48, Step 50, Loss: 1.9925\n","Epoch 48, Step 60, Loss: 0.0353\n","Epoch 48, Step 70, Loss: 0.0609\n","Epoch 48, Step 80, Loss: 1.4281\n","Epoch 48, Step 90, Loss: 0.0557\n","Epoch 48, Step 100, Loss: 0.0698\n","Epoch 48, Step 110, Loss: 0.0465\n","Epoch 48, Step 120, Loss: 0.0725\n","Epoch 48, Step 130, Loss: 0.0513\n","Epoch 48, Step 140, Loss: 0.0500\n","Epoch 48, Step 150, Loss: 0.0464\n","Average Loss in Epoch 48: 0.33408234441805185\n","-------------------------------------------------\n","Epoch 49, Step 10, Loss: 0.0563\n","Epoch 49, Step 20, Loss: 0.0445\n","Epoch 49, Step 30, Loss: 0.0561\n","Epoch 49, Step 40, Loss: 0.0797\n","Epoch 49, Step 50, Loss: 0.0749\n","Epoch 49, Step 60, Loss: 0.0780\n","Epoch 49, Step 70, Loss: 0.0754\n","Epoch 49, Step 80, Loss: 1.2733\n","Epoch 49, Step 90, Loss: 0.0512\n","Epoch 49, Step 100, Loss: 0.0776\n","Epoch 49, Step 110, Loss: 0.0484\n","Epoch 49, Step 120, Loss: 0.7568\n","Epoch 49, Step 130, Loss: 0.0912\n","Epoch 49, Step 140, Loss: 0.0793\n","Epoch 49, Step 150, Loss: 0.0899\n","Average Loss in Epoch 49: 0.3338284713901439\n","-------------------------------------------------\n","Epoch 50, Step 10, Loss: 0.0684\n","Epoch 50, Step 20, Loss: 0.0569\n","Epoch 50, Step 30, Loss: 0.0951\n","Epoch 50, Step 40, Loss: 0.0823\n","Epoch 50, Step 50, Loss: 1.2967\n","Epoch 50, Step 60, Loss: 1.3862\n","Epoch 50, Step 70, Loss: 0.0614\n","Epoch 50, Step 80, Loss: 0.0447\n","Epoch 50, Step 90, Loss: 0.9588\n","Epoch 50, Step 100, Loss: 0.0563\n","Epoch 50, Step 110, Loss: 0.0696\n","Epoch 50, Step 120, Loss: 0.0632\n","Epoch 50, Step 130, Loss: 0.0384\n","Epoch 50, Step 140, Loss: 0.0553\n","Epoch 50, Step 150, Loss: 0.0958\n","Average Loss in Epoch 50: 0.33346540527711127\n","-------------------------------------------------\n","Epoch 51, Step 10, Loss: 0.0566\n","Epoch 51, Step 20, Loss: 1.5637\n","Epoch 51, Step 30, Loss: 1.2112\n","Epoch 51, Step 40, Loss: 0.0386\n","Epoch 51, Step 50, Loss: 1.1777\n","Epoch 51, Step 60, Loss: 0.1100\n","Epoch 51, Step 70, Loss: 0.0969\n","Epoch 51, Step 80, Loss: 0.0825\n","Epoch 51, Step 90, Loss: 2.2627\n","Epoch 51, Step 100, Loss: 0.0533\n","Epoch 51, Step 110, Loss: 1.4744\n","Epoch 51, Step 120, Loss: 0.0411\n","Epoch 51, Step 130, Loss: 0.0461\n","Epoch 51, Step 140, Loss: 0.0704\n","Epoch 51, Step 150, Loss: 0.0656\n","Average Loss in Epoch 51: 0.33340135661873427\n","-------------------------------------------------\n","Epoch 52, Step 10, Loss: 0.0483\n","Epoch 52, Step 20, Loss: 0.0487\n","Epoch 52, Step 30, Loss: 0.0995\n","Epoch 52, Step 40, Loss: 1.0896\n","Epoch 52, Step 50, Loss: 0.0383\n","Epoch 52, Step 60, Loss: 1.9688\n","Epoch 52, Step 70, Loss: 0.0505\n","Epoch 52, Step 80, Loss: 0.6502\n","Epoch 52, Step 90, Loss: 0.7530\n","Epoch 52, Step 100, Loss: 0.0697\n","Epoch 52, Step 110, Loss: 1.5158\n","Epoch 52, Step 120, Loss: 0.0813\n","Epoch 52, Step 130, Loss: 1.3748\n","Epoch 52, Step 140, Loss: 0.0623\n","Epoch 52, Step 150, Loss: 0.0610\n","Average Loss in Epoch 52: 0.3330170696355262\n","-------------------------------------------------\n","Epoch 53, Step 10, Loss: 0.0767\n","Epoch 53, Step 20, Loss: 0.0490\n","Epoch 53, Step 30, Loss: 0.8281\n","Epoch 53, Step 40, Loss: 1.1766\n","Epoch 53, Step 50, Loss: 0.0667\n","Epoch 53, Step 60, Loss: 1.5017\n","Epoch 53, Step 70, Loss: 0.0830\n","Epoch 53, Step 80, Loss: 0.0633\n","Epoch 53, Step 90, Loss: 0.0370\n","Epoch 53, Step 100, Loss: 1.3928\n","Epoch 53, Step 110, Loss: 1.5188\n","Epoch 53, Step 120, Loss: 0.0353\n","Epoch 53, Step 130, Loss: 0.0860\n","Epoch 53, Step 140, Loss: 0.0952\n","Epoch 53, Step 150, Loss: 0.0582\n","Average Loss in Epoch 53: 0.33274376638374237\n","-------------------------------------------------\n","Epoch 54, Step 10, Loss: 0.0574\n","Epoch 54, Step 20, Loss: 0.0454\n","Epoch 54, Step 30, Loss: 0.0394\n","Epoch 54, Step 40, Loss: 0.1013\n","Epoch 54, Step 50, Loss: 0.0515\n","Epoch 54, Step 60, Loss: 0.0537\n","Epoch 54, Step 70, Loss: 0.0613\n","Epoch 54, Step 80, Loss: 1.2932\n","Epoch 54, Step 90, Loss: 0.0707\n","Epoch 54, Step 100, Loss: 0.0601\n","Epoch 54, Step 110, Loss: 0.0816\n","Epoch 54, Step 120, Loss: 0.0726\n","Epoch 54, Step 130, Loss: 0.0462\n","Epoch 54, Step 140, Loss: 0.4812\n","Epoch 54, Step 150, Loss: 1.3993\n","Average Loss in Epoch 54: 0.3324653331846374\n","-------------------------------------------------\n","Epoch 55, Step 10, Loss: 0.0503\n","Epoch 55, Step 20, Loss: 0.0596\n","Epoch 55, Step 30, Loss: 0.0614\n","Epoch 55, Step 40, Loss: 0.8926\n","Epoch 55, Step 50, Loss: 0.1077\n","Epoch 55, Step 60, Loss: 1.0200\n","Epoch 55, Step 70, Loss: 0.0562\n","Epoch 55, Step 80, Loss: 0.6719\n","Epoch 55, Step 90, Loss: 0.0603\n","Epoch 55, Step 100, Loss: 1.4837\n","Epoch 55, Step 110, Loss: 0.0925\n","Epoch 55, Step 120, Loss: 0.8388\n","Epoch 55, Step 130, Loss: 0.0555\n","Epoch 55, Step 140, Loss: 1.6798\n","Epoch 55, Step 150, Loss: 0.0703\n","Average Loss in Epoch 55: 0.33224730769980626\n","-------------------------------------------------\n","Epoch 56, Step 10, Loss: 0.0397\n","Epoch 56, Step 20, Loss: 0.0527\n","Epoch 56, Step 30, Loss: 0.0573\n","Epoch 56, Step 40, Loss: 1.1711\n","Epoch 56, Step 50, Loss: 0.0614\n","Epoch 56, Step 60, Loss: 1.1982\n","Epoch 56, Step 70, Loss: 0.6499\n","Epoch 56, Step 80, Loss: 0.6912\n","Epoch 56, Step 90, Loss: 0.0809\n","Epoch 56, Step 100, Loss: 0.0684\n","Epoch 56, Step 110, Loss: 0.0587\n","Epoch 56, Step 120, Loss: 0.0645\n","Epoch 56, Step 130, Loss: 0.0872\n","Epoch 56, Step 140, Loss: 1.8456\n","Epoch 56, Step 150, Loss: 0.6822\n","Average Loss in Epoch 56: 0.3320728359779097\n","-------------------------------------------------\n","Epoch 57, Step 10, Loss: 1.3625\n","Epoch 57, Step 20, Loss: 0.0425\n","Epoch 57, Step 30, Loss: 0.0520\n","Epoch 57, Step 40, Loss: 0.0499\n","Epoch 57, Step 50, Loss: 0.0436\n","Epoch 57, Step 60, Loss: 0.0717\n","Epoch 57, Step 70, Loss: 0.8776\n","Epoch 57, Step 80, Loss: 0.0594\n","Epoch 57, Step 90, Loss: 0.0660\n","Epoch 57, Step 100, Loss: 0.0504\n","Epoch 57, Step 110, Loss: 0.4775\n","Epoch 57, Step 120, Loss: 0.0536\n","Epoch 57, Step 130, Loss: 0.0472\n","Epoch 57, Step 140, Loss: 0.0509\n","Epoch 57, Step 150, Loss: 0.0558\n","Average Loss in Epoch 57: 0.3253502904103612\n","-------------------------------------------------\n","Epoch 58, Step 10, Loss: 0.9882\n","Epoch 58, Step 20, Loss: 0.8504\n","Epoch 58, Step 30, Loss: 1.3888\n","Epoch 58, Step 40, Loss: 0.0609\n","Epoch 58, Step 50, Loss: 0.0548\n","Epoch 58, Step 60, Loss: 1.2773\n","Epoch 58, Step 70, Loss: 3.4609\n","Epoch 58, Step 80, Loss: 0.0458\n","Epoch 58, Step 90, Loss: 0.0578\n","Epoch 58, Step 100, Loss: 0.0464\n","Epoch 58, Step 110, Loss: 0.0523\n","Epoch 58, Step 120, Loss: 0.0460\n","Epoch 58, Step 130, Loss: 1.8197\n","Epoch 58, Step 140, Loss: 0.0536\n","Epoch 58, Step 150, Loss: 0.0542\n","Average Loss in Epoch 58: 0.3314211161326875\n","-------------------------------------------------\n","Epoch 59, Step 10, Loss: 0.4781\n","Epoch 59, Step 20, Loss: 0.0503\n","Epoch 59, Step 30, Loss: 0.0717\n","Epoch 59, Step 40, Loss: 0.0435\n","Epoch 59, Step 50, Loss: 0.0483\n","Epoch 59, Step 60, Loss: 0.0414\n","Epoch 59, Step 70, Loss: 0.0775\n","Epoch 59, Step 80, Loss: 0.0890\n","Epoch 59, Step 90, Loss: 0.0640\n","Epoch 59, Step 100, Loss: 0.0959\n","Epoch 59, Step 110, Loss: 0.0660\n","Epoch 59, Step 120, Loss: 0.0612\n","Epoch 59, Step 130, Loss: 0.0506\n","Epoch 59, Step 140, Loss: 1.6621\n","Epoch 59, Step 150, Loss: 0.0583\n","Average Loss in Epoch 59: 0.33135151710806404\n","-------------------------------------------------\n","Epoch 60, Step 10, Loss: 0.8431\n","Epoch 60, Step 20, Loss: 0.0618\n","Epoch 60, Step 30, Loss: 0.0368\n","Epoch 60, Step 40, Loss: 0.0952\n","Epoch 60, Step 50, Loss: 0.0694\n","Epoch 60, Step 60, Loss: 0.8426\n","Epoch 60, Step 70, Loss: 0.0431\n","Epoch 60, Step 80, Loss: 0.0322\n","Epoch 60, Step 90, Loss: 0.0615\n","Epoch 60, Step 100, Loss: 0.0504\n","Epoch 60, Step 110, Loss: 0.0682\n","Epoch 60, Step 120, Loss: 0.0553\n","Epoch 60, Step 130, Loss: 0.0611\n","Epoch 60, Step 140, Loss: 0.5419\n","Epoch 60, Step 150, Loss: 1.9802\n","Average Loss in Epoch 60: 0.33112571964848714\n","-------------------------------------------------\n","Epoch 61, Step 10, Loss: 0.0435\n","Epoch 61, Step 20, Loss: 1.2238\n","Epoch 61, Step 30, Loss: 0.0572\n","Epoch 61, Step 40, Loss: 0.0804\n","Epoch 61, Step 50, Loss: 0.0622\n","Epoch 61, Step 60, Loss: 0.0423\n","Epoch 61, Step 70, Loss: 0.0862\n","Epoch 61, Step 80, Loss: 0.0627\n","Epoch 61, Step 90, Loss: 0.0700\n","Epoch 61, Step 100, Loss: 0.0462\n","Epoch 61, Step 110, Loss: 0.0435\n","Epoch 61, Step 120, Loss: 0.0538\n","Epoch 61, Step 130, Loss: 0.0652\n","Epoch 61, Step 140, Loss: 0.0614\n","Epoch 61, Step 150, Loss: 0.0474\n","Average Loss in Epoch 61: 0.32337448931059\n","-------------------------------------------------\n","Epoch 62, Step 10, Loss: 0.0598\n","Epoch 62, Step 20, Loss: 0.9476\n","Epoch 62, Step 30, Loss: 0.0467\n","Epoch 62, Step 40, Loss: 0.0472\n","Epoch 62, Step 50, Loss: 0.0179\n","Epoch 62, Step 60, Loss: 0.0772\n","Epoch 62, Step 70, Loss: 0.0480\n","Epoch 62, Step 80, Loss: 0.0689\n","Epoch 62, Step 90, Loss: 0.0440\n","Epoch 62, Step 100, Loss: 0.4746\n","Epoch 62, Step 110, Loss: 0.0537\n","Epoch 62, Step 120, Loss: 0.0539\n","Epoch 62, Step 130, Loss: 0.0828\n","Epoch 62, Step 140, Loss: 2.5854\n","Epoch 62, Step 150, Loss: 0.0763\n","Average Loss in Epoch 62: 0.3307431490634972\n","-------------------------------------------------\n","Epoch 63, Step 10, Loss: 1.4987\n","Epoch 63, Step 20, Loss: 0.0549\n","Epoch 63, Step 30, Loss: 0.0750\n","Epoch 63, Step 40, Loss: 0.0592\n","Epoch 63, Step 50, Loss: 1.2147\n","Epoch 63, Step 60, Loss: 0.0349\n","Epoch 63, Step 70, Loss: 1.6974\n","Epoch 63, Step 80, Loss: 0.0498\n","Epoch 63, Step 90, Loss: 0.0569\n","Epoch 63, Step 100, Loss: 0.0913\n","Epoch 63, Step 110, Loss: 0.0582\n","Epoch 63, Step 120, Loss: 0.0669\n","Epoch 63, Step 130, Loss: 1.4074\n","Epoch 63, Step 140, Loss: 0.0554\n","Epoch 63, Step 150, Loss: 0.0674\n","Average Loss in Epoch 63: 0.3305149516969357\n","-------------------------------------------------\n","Epoch 64, Step 10, Loss: 1.2896\n","Epoch 64, Step 20, Loss: 0.0376\n","Epoch 64, Step 30, Loss: 0.0544\n","Epoch 64, Step 40, Loss: 0.0612\n","Epoch 64, Step 50, Loss: 0.6252\n","Epoch 64, Step 60, Loss: 0.9662\n","Epoch 64, Step 70, Loss: 0.0605\n","Epoch 64, Step 80, Loss: 0.0713\n","Epoch 64, Step 90, Loss: 0.0739\n","Epoch 64, Step 100, Loss: 0.0977\n","Epoch 64, Step 110, Loss: 0.0643\n","Epoch 64, Step 120, Loss: 0.0465\n","Epoch 64, Step 130, Loss: 0.4761\n","Epoch 64, Step 140, Loss: 0.0839\n","Epoch 64, Step 150, Loss: 0.0600\n","Average Loss in Epoch 64: 0.3302848055583876\n","-------------------------------------------------\n","Epoch 65, Step 10, Loss: 0.0423\n","Epoch 65, Step 20, Loss: 0.0281\n","Epoch 65, Step 30, Loss: 0.0388\n","Epoch 65, Step 40, Loss: 0.0639\n","Epoch 65, Step 50, Loss: 0.6522\n","Epoch 65, Step 60, Loss: 0.0764\n","Epoch 65, Step 70, Loss: 0.0780\n","Epoch 65, Step 80, Loss: 0.0482\n","Epoch 65, Step 90, Loss: 0.0583\n","Epoch 65, Step 100, Loss: 0.0522\n","Epoch 65, Step 110, Loss: 1.3862\n","Epoch 65, Step 120, Loss: 0.0461\n","Epoch 65, Step 130, Loss: 0.0488\n","Epoch 65, Step 140, Loss: 0.0615\n","Epoch 65, Step 150, Loss: 0.0589\n","Average Loss in Epoch 65: 0.330272570838156\n","-------------------------------------------------\n","Epoch 66, Step 10, Loss: 0.0669\n","Epoch 66, Step 20, Loss: 0.9643\n","Epoch 66, Step 30, Loss: 1.6905\n","Epoch 66, Step 40, Loss: 0.0532\n","Epoch 66, Step 50, Loss: 0.0642\n","Epoch 66, Step 60, Loss: 0.7730\n","Epoch 66, Step 70, Loss: 1.6065\n","Epoch 66, Step 80, Loss: 0.6504\n","Epoch 66, Step 90, Loss: 0.0519\n","Epoch 66, Step 100, Loss: 0.0535\n","Epoch 66, Step 110, Loss: 1.3875\n","Epoch 66, Step 120, Loss: 0.0839\n","Epoch 66, Step 130, Loss: 0.0545\n","Epoch 66, Step 140, Loss: 2.9387\n","Epoch 66, Step 150, Loss: 0.0690\n","Average Loss in Epoch 66: 0.319690961004147\n","-------------------------------------------------\n","Epoch 67, Step 10, Loss: 0.0474\n","Epoch 67, Step 20, Loss: 0.0460\n","Epoch 67, Step 30, Loss: 0.0481\n","Epoch 67, Step 40, Loss: 0.0609\n","Epoch 67, Step 50, Loss: 0.0501\n","Epoch 67, Step 60, Loss: 0.0629\n","Epoch 67, Step 70, Loss: 0.0791\n","Epoch 67, Step 80, Loss: 0.0545\n","Epoch 67, Step 90, Loss: 0.0466\n","Epoch 67, Step 100, Loss: 1.2440\n","Epoch 67, Step 110, Loss: 0.0644\n","Epoch 67, Step 120, Loss: 0.0741\n","Epoch 67, Step 130, Loss: 0.0801\n","Epoch 67, Step 140, Loss: 0.0677\n","Epoch 67, Step 150, Loss: 0.0789\n","Average Loss in Epoch 67: 0.32981921748826337\n","-------------------------------------------------\n","Epoch 68, Step 10, Loss: 0.0292\n","Epoch 68, Step 20, Loss: 0.0566\n","Epoch 68, Step 30, Loss: 0.0634\n","Epoch 68, Step 40, Loss: 0.0637\n","Epoch 68, Step 50, Loss: 0.0625\n","Epoch 68, Step 60, Loss: 0.0605\n","Epoch 68, Step 70, Loss: 0.0775\n","Epoch 68, Step 80, Loss: 0.0454\n","Epoch 68, Step 90, Loss: 0.0428\n","Epoch 68, Step 100, Loss: 0.0543\n","Epoch 68, Step 110, Loss: 0.0602\n","Epoch 68, Step 120, Loss: 0.0438\n","Epoch 68, Step 130, Loss: 3.0089\n","Epoch 68, Step 140, Loss: 0.0351\n","Epoch 68, Step 150, Loss: 0.0465\n","Average Loss in Epoch 68: 0.3296289095690228\n","-------------------------------------------------\n","Epoch 69, Step 10, Loss: 0.0482\n","Epoch 69, Step 20, Loss: 0.0578\n","Epoch 69, Step 30, Loss: 0.0406\n","Epoch 69, Step 40, Loss: 0.0458\n","Epoch 69, Step 50, Loss: 0.0594\n","Epoch 69, Step 60, Loss: 0.0573\n","Epoch 69, Step 70, Loss: 1.5631\n","Epoch 69, Step 80, Loss: 0.0436\n","Epoch 69, Step 90, Loss: 1.8135\n","Epoch 69, Step 100, Loss: 0.0670\n","Epoch 69, Step 110, Loss: 0.0435\n","Epoch 69, Step 120, Loss: 0.0765\n","Epoch 69, Step 130, Loss: 0.0678\n","Epoch 69, Step 140, Loss: 0.0627\n","Epoch 69, Step 150, Loss: 0.0642\n","Average Loss in Epoch 69: 0.3295417891217853\n","-------------------------------------------------\n","Epoch 70, Step 10, Loss: 0.5002\n","Epoch 70, Step 20, Loss: 0.0535\n","Epoch 70, Step 30, Loss: 1.3751\n","Epoch 70, Step 40, Loss: 0.0601\n","Epoch 70, Step 50, Loss: 1.1897\n","Epoch 70, Step 60, Loss: 0.0945\n","Epoch 70, Step 70, Loss: 0.0635\n","Epoch 70, Step 80, Loss: 0.0509\n","Epoch 70, Step 90, Loss: 0.0905\n","Epoch 70, Step 100, Loss: 0.0770\n","Epoch 70, Step 110, Loss: 1.8157\n","Epoch 70, Step 120, Loss: 0.0542\n","Epoch 70, Step 130, Loss: 0.0573\n","Epoch 70, Step 140, Loss: 0.0687\n","Epoch 70, Step 150, Loss: 0.0724\n","Average Loss in Epoch 70: 0.3294244220937198\n","-------------------------------------------------\n","Epoch 71, Step 10, Loss: 0.0611\n","Epoch 71, Step 20, Loss: 0.0767\n","Epoch 71, Step 30, Loss: 0.0928\n","Epoch 71, Step 40, Loss: 1.3154\n","Epoch 71, Step 50, Loss: 0.0520\n","Epoch 71, Step 60, Loss: 0.0582\n","Epoch 71, Step 70, Loss: 0.0816\n","Epoch 71, Step 80, Loss: 1.7255\n","Epoch 71, Step 90, Loss: 0.0382\n","Epoch 71, Step 100, Loss: 0.0422\n","Epoch 71, Step 110, Loss: 0.0582\n","Epoch 71, Step 120, Loss: 0.0706\n","Epoch 71, Step 130, Loss: 0.0588\n","Epoch 71, Step 140, Loss: 0.0464\n","Epoch 71, Step 150, Loss: 0.0769\n","Average Loss in Epoch 71: 0.32924052801147197\n","-------------------------------------------------\n","Epoch 72, Step 10, Loss: 1.3167\n","Epoch 72, Step 20, Loss: 0.0484\n","Epoch 72, Step 30, Loss: 1.4323\n","Epoch 72, Step 40, Loss: 0.6831\n","Epoch 72, Step 50, Loss: 0.0428\n","Epoch 72, Step 60, Loss: 0.0707\n","Epoch 72, Step 70, Loss: 1.1833\n","Epoch 72, Step 80, Loss: 0.0524\n","Epoch 72, Step 90, Loss: 0.0676\n","Epoch 72, Step 100, Loss: 0.0384\n","Epoch 72, Step 110, Loss: 0.0733\n","Epoch 72, Step 120, Loss: 0.0502\n","Epoch 72, Step 130, Loss: 0.0736\n","Epoch 72, Step 140, Loss: 0.0405\n","Epoch 72, Step 150, Loss: 0.8392\n","Average Loss in Epoch 72: 0.3254571290651582\n","-------------------------------------------------\n","Epoch 73, Step 10, Loss: 0.0601\n","Epoch 73, Step 20, Loss: 0.0542\n","Epoch 73, Step 30, Loss: 1.8129\n","Epoch 73, Step 40, Loss: 0.0493\n","Epoch 73, Step 50, Loss: 0.8633\n","Epoch 73, Step 60, Loss: 0.0476\n","Epoch 73, Step 70, Loss: 0.0624\n","Epoch 73, Step 80, Loss: 0.0548\n","Epoch 73, Step 90, Loss: 0.0570\n","Epoch 73, Step 100, Loss: 0.0650\n","Epoch 73, Step 110, Loss: 0.0572\n","Epoch 73, Step 120, Loss: 0.0575\n","Epoch 73, Step 130, Loss: 0.0653\n","Epoch 73, Step 140, Loss: 0.0517\n","Epoch 73, Step 150, Loss: 0.0653\n","Average Loss in Epoch 73: 0.32891133446363535\n","-------------------------------------------------\n","Epoch 74, Step 10, Loss: 1.3790\n","Epoch 74, Step 20, Loss: 0.0453\n","Epoch 74, Step 30, Loss: 0.0440\n","Epoch 74, Step 40, Loss: 0.0710\n","Epoch 74, Step 50, Loss: 0.0402\n","Epoch 74, Step 60, Loss: 0.0685\n","Epoch 74, Step 70, Loss: 0.0420\n","Epoch 74, Step 80, Loss: 0.0542\n","Epoch 74, Step 90, Loss: 0.0994\n","Epoch 74, Step 100, Loss: 1.5519\n","Epoch 74, Step 110, Loss: 0.0679\n","Epoch 74, Step 120, Loss: 0.0460\n","Epoch 74, Step 130, Loss: 0.9667\n","Epoch 74, Step 140, Loss: 1.1933\n","Epoch 74, Step 150, Loss: 0.0548\n","Average Loss in Epoch 74: 0.328781455743519\n","-------------------------------------------------\n","Epoch 75, Step 10, Loss: 0.0240\n","Epoch 75, Step 20, Loss: 0.0635\n","Epoch 75, Step 30, Loss: 1.0848\n","Epoch 75, Step 40, Loss: 1.8367\n","Epoch 75, Step 50, Loss: 0.0656\n","Epoch 75, Step 60, Loss: 0.5105\n","Epoch 75, Step 70, Loss: 0.4982\n","Epoch 75, Step 80, Loss: 0.0800\n","Epoch 75, Step 90, Loss: 0.0364\n","Epoch 75, Step 100, Loss: 0.0601\n","Epoch 75, Step 110, Loss: 0.6412\n","Epoch 75, Step 120, Loss: 0.0582\n","Epoch 75, Step 130, Loss: 0.0595\n","Epoch 75, Step 140, Loss: 0.0500\n","Epoch 75, Step 150, Loss: 0.0621\n","Average Loss in Epoch 75: 0.32854043456883925\n","-------------------------------------------------\n","Epoch 76, Step 10, Loss: 0.0521\n","Epoch 76, Step 20, Loss: 0.0436\n","Epoch 76, Step 30, Loss: 1.8264\n","Epoch 76, Step 40, Loss: 0.0830\n","Epoch 76, Step 50, Loss: 0.0589\n","Epoch 76, Step 60, Loss: 1.4312\n","Epoch 76, Step 70, Loss: 0.0495\n","Epoch 76, Step 80, Loss: 1.2692\n","Epoch 76, Step 90, Loss: 0.0592\n","Epoch 76, Step 100, Loss: 0.0639\n","Epoch 76, Step 110, Loss: 1.2168\n","Epoch 76, Step 120, Loss: 0.0730\n","Epoch 76, Step 130, Loss: 0.8900\n","Epoch 76, Step 140, Loss: 0.0532\n","Epoch 76, Step 150, Loss: 1.4700\n","Average Loss in Epoch 76: 0.32848225442587203\n","-------------------------------------------------\n","Epoch 77, Step 10, Loss: 0.0460\n","Epoch 77, Step 20, Loss: 0.6887\n","Epoch 77, Step 30, Loss: 0.0725\n","Epoch 77, Step 40, Loss: 0.0643\n","Epoch 77, Step 50, Loss: 0.0340\n","Epoch 77, Step 60, Loss: 0.0542\n","Epoch 77, Step 70, Loss: 1.4698\n","Epoch 77, Step 80, Loss: 0.0570\n","Epoch 77, Step 90, Loss: 0.0749\n","Epoch 77, Step 100, Loss: 1.2789\n","Epoch 77, Step 110, Loss: 0.0663\n","Epoch 77, Step 120, Loss: 0.0558\n","Epoch 77, Step 130, Loss: 0.0726\n","Epoch 77, Step 140, Loss: 0.0335\n","Epoch 77, Step 150, Loss: 0.0517\n","Average Loss in Epoch 77: 0.3282667901601244\n","-------------------------------------------------\n","Epoch 78, Step 10, Loss: 0.0797\n","Epoch 78, Step 20, Loss: 0.0275\n","Epoch 78, Step 30, Loss: 0.0592\n","Epoch 78, Step 40, Loss: 0.0625\n","Epoch 78, Step 50, Loss: 0.0608\n","Epoch 78, Step 60, Loss: 1.5394\n","Epoch 78, Step 70, Loss: 0.9245\n","Epoch 78, Step 80, Loss: 0.0716\n","Epoch 78, Step 90, Loss: 0.4737\n","Epoch 78, Step 100, Loss: 0.0648\n","Epoch 78, Step 110, Loss: 1.3859\n","Epoch 78, Step 120, Loss: 0.0743\n","Epoch 78, Step 130, Loss: 0.8587\n","Epoch 78, Step 140, Loss: 1.1864\n","Epoch 78, Step 150, Loss: 0.0416\n","Average Loss in Epoch 78: 0.32814726099934216\n","-------------------------------------------------\n","Epoch 79, Step 10, Loss: 1.9486\n","Epoch 79, Step 20, Loss: 0.8441\n","Epoch 79, Step 30, Loss: 0.0821\n","Epoch 79, Step 40, Loss: 2.5860\n","Epoch 79, Step 50, Loss: 0.0685\n","Epoch 79, Step 60, Loss: 0.0762\n","Epoch 79, Step 70, Loss: 0.8364\n","Epoch 79, Step 80, Loss: 0.0448\n","Epoch 79, Step 90, Loss: 0.0618\n","Epoch 79, Step 100, Loss: 0.0507\n","Epoch 79, Step 110, Loss: 0.0651\n","Epoch 79, Step 120, Loss: 0.0404\n","Epoch 79, Step 130, Loss: 0.8212\n","Epoch 79, Step 140, Loss: 0.0846\n","Epoch 79, Step 150, Loss: 0.0792\n","Average Loss in Epoch 79: 0.3279300631849833\n","-------------------------------------------------\n","Epoch 80, Step 10, Loss: 0.0513\n","Epoch 80, Step 20, Loss: 0.0498\n","Epoch 80, Step 30, Loss: 0.0850\n","Epoch 80, Step 40, Loss: 0.0393\n","Epoch 80, Step 50, Loss: 0.0747\n","Epoch 80, Step 60, Loss: 0.0500\n","Epoch 80, Step 70, Loss: 1.2932\n","Epoch 80, Step 80, Loss: 0.0753\n","Epoch 80, Step 90, Loss: 0.0823\n","Epoch 80, Step 100, Loss: 0.0580\n","Epoch 80, Step 110, Loss: 0.0544\n","Epoch 80, Step 120, Loss: 0.0924\n","Epoch 80, Step 130, Loss: 1.4468\n","Epoch 80, Step 140, Loss: 0.0482\n","Epoch 80, Step 150, Loss: 0.0523\n","Average Loss in Epoch 80: 0.32769127706733514\n","-------------------------------------------------\n","Epoch 81, Step 10, Loss: 0.0769\n","Epoch 81, Step 20, Loss: 2.8674\n","Epoch 81, Step 30, Loss: 0.6033\n","Epoch 81, Step 40, Loss: 1.8092\n","Epoch 81, Step 50, Loss: 1.4891\n","Epoch 81, Step 60, Loss: 0.0675\n","Epoch 81, Step 70, Loss: 1.7046\n","Epoch 81, Step 80, Loss: 0.0865\n","Epoch 81, Step 90, Loss: 0.0501\n","Epoch 81, Step 100, Loss: 0.0595\n","Epoch 81, Step 110, Loss: 0.0649\n","Epoch 81, Step 120, Loss: 0.0628\n","Epoch 81, Step 130, Loss: 0.0653\n","Epoch 81, Step 140, Loss: 0.0387\n","Epoch 81, Step 150, Loss: 0.0509\n","Average Loss in Epoch 81: 0.32771279126312\n","-------------------------------------------------\n","Epoch 82, Step 10, Loss: 0.0764\n","Epoch 82, Step 20, Loss: 1.8291\n","Epoch 82, Step 30, Loss: 1.0584\n","Epoch 82, Step 40, Loss: 1.4379\n","Epoch 82, Step 50, Loss: 1.2667\n","Epoch 82, Step 60, Loss: 0.0754\n","Epoch 82, Step 70, Loss: 0.0600\n","Epoch 82, Step 80, Loss: 0.0588\n","Epoch 82, Step 90, Loss: 0.0556\n","Epoch 82, Step 100, Loss: 0.0464\n","Epoch 82, Step 110, Loss: 0.0680\n","Epoch 82, Step 120, Loss: 0.0353\n","Epoch 82, Step 130, Loss: 0.0573\n","Epoch 82, Step 140, Loss: 0.0947\n","Epoch 82, Step 150, Loss: 0.0372\n","Average Loss in Epoch 82: 0.32760775584797813\n","-------------------------------------------------\n","Epoch 83, Step 10, Loss: 0.0439\n","Epoch 83, Step 20, Loss: 1.2234\n","Epoch 83, Step 30, Loss: 0.0476\n","Epoch 83, Step 40, Loss: 0.0736\n","Epoch 83, Step 50, Loss: 0.0514\n","Epoch 83, Step 60, Loss: 0.0622\n","Epoch 83, Step 70, Loss: 0.0566\n","Epoch 83, Step 80, Loss: 0.0580\n","Epoch 83, Step 90, Loss: 0.6862\n","Epoch 83, Step 100, Loss: 0.0232\n","Epoch 83, Step 110, Loss: 0.0713\n","Epoch 83, Step 120, Loss: 0.0808\n","Epoch 83, Step 130, Loss: 0.0724\n","Epoch 83, Step 140, Loss: 0.0461\n","Epoch 83, Step 150, Loss: 0.0660\n","Average Loss in Epoch 83: 0.3274554176923801\n","-------------------------------------------------\n","Epoch 84, Step 10, Loss: 0.0591\n","Epoch 84, Step 20, Loss: 0.0645\n","Epoch 84, Step 30, Loss: 0.0630\n","Epoch 84, Step 40, Loss: 0.0538\n","Epoch 84, Step 50, Loss: 0.0748\n","Epoch 84, Step 60, Loss: 0.0738\n","Epoch 84, Step 70, Loss: 0.0399\n","Epoch 84, Step 80, Loss: 1.2902\n","Epoch 84, Step 90, Loss: 0.9696\n","Epoch 84, Step 100, Loss: 0.0509\n","Epoch 84, Step 110, Loss: 0.0711\n","Epoch 84, Step 120, Loss: 0.0436\n","Epoch 84, Step 130, Loss: 0.0314\n","Epoch 84, Step 140, Loss: 0.0542\n","Epoch 84, Step 150, Loss: 0.0637\n","Average Loss in Epoch 84: 0.3272495538160299\n","-------------------------------------------------\n","Epoch 85, Step 10, Loss: 0.9666\n","Epoch 85, Step 20, Loss: 0.0522\n","Epoch 85, Step 30, Loss: 0.0524\n","Epoch 85, Step 40, Loss: 0.0722\n","Epoch 85, Step 50, Loss: 0.0542\n","Epoch 85, Step 60, Loss: 0.0739\n","Epoch 85, Step 70, Loss: 0.0490\n","Epoch 85, Step 80, Loss: 0.0458\n","Epoch 85, Step 90, Loss: 0.0966\n","Epoch 85, Step 100, Loss: 1.0660\n","Epoch 85, Step 110, Loss: 0.0590\n","Epoch 85, Step 120, Loss: 0.0666\n","Epoch 85, Step 130, Loss: 0.0644\n","Epoch 85, Step 140, Loss: 0.0781\n","Epoch 85, Step 150, Loss: 0.0455\n","Average Loss in Epoch 85: 0.3271888376758346\n","-------------------------------------------------\n","Epoch 86, Step 10, Loss: 0.0434\n","Epoch 86, Step 20, Loss: 0.0541\n","Epoch 86, Step 30, Loss: 0.0446\n","Epoch 86, Step 40, Loss: 0.7510\n","Epoch 86, Step 50, Loss: 0.0474\n","Epoch 86, Step 60, Loss: 0.0700\n","Epoch 86, Step 70, Loss: 0.0309\n","Epoch 86, Step 80, Loss: 1.7941\n","Epoch 86, Step 90, Loss: 0.0760\n","Epoch 86, Step 100, Loss: 0.0561\n","Epoch 86, Step 110, Loss: 0.0488\n","Epoch 86, Step 120, Loss: 0.1151\n","Epoch 86, Step 130, Loss: 0.0628\n","Epoch 86, Step 140, Loss: 0.5463\n","Epoch 86, Step 150, Loss: 0.0705\n","Average Loss in Epoch 86: 0.3271671547173704\n","-------------------------------------------------\n","Epoch 87, Step 10, Loss: 0.0806\n","Epoch 87, Step 20, Loss: 0.0333\n","Epoch 87, Step 30, Loss: 0.9311\n","Epoch 87, Step 40, Loss: 0.0408\n","Epoch 87, Step 50, Loss: 0.0826\n","Epoch 87, Step 60, Loss: 0.0527\n","Epoch 87, Step 70, Loss: 0.0613\n","Epoch 87, Step 80, Loss: 0.0742\n","Epoch 87, Step 90, Loss: 0.0350\n","Epoch 87, Step 100, Loss: 0.0511\n","Epoch 87, Step 110, Loss: 0.0571\n","Epoch 87, Step 120, Loss: 0.0370\n","Epoch 87, Step 130, Loss: 0.0666\n","Epoch 87, Step 140, Loss: 0.0491\n","Epoch 87, Step 150, Loss: 0.0874\n","Average Loss in Epoch 87: 0.3269739942449444\n","-------------------------------------------------\n","Epoch 88, Step 10, Loss: 0.9513\n","Epoch 88, Step 20, Loss: 0.0670\n","Epoch 88, Step 30, Loss: 0.0728\n","Epoch 88, Step 40, Loss: 0.6874\n","Epoch 88, Step 50, Loss: 1.6868\n","Epoch 88, Step 60, Loss: 1.3956\n","Epoch 88, Step 70, Loss: 0.0682\n","Epoch 88, Step 80, Loss: 0.0643\n","Epoch 88, Step 90, Loss: 0.0567\n","Epoch 88, Step 100, Loss: 0.6621\n","Epoch 88, Step 110, Loss: 0.0728\n","Epoch 88, Step 120, Loss: 0.0608\n","Epoch 88, Step 130, Loss: 1.2446\n","Epoch 88, Step 140, Loss: 0.0333\n","Epoch 88, Step 150, Loss: 0.0889\n","Average Loss in Epoch 88: 0.3149711422200473\n","-------------------------------------------------\n","Epoch 89, Step 10, Loss: 0.0652\n","Epoch 89, Step 20, Loss: 1.5291\n","Epoch 89, Step 30, Loss: 0.0967\n","Epoch 89, Step 40, Loss: 0.0438\n","Epoch 89, Step 50, Loss: 0.0770\n","Epoch 89, Step 60, Loss: 0.0475\n","Epoch 89, Step 70, Loss: 0.8130\n","Epoch 89, Step 80, Loss: 0.0354\n","Epoch 89, Step 90, Loss: 0.0574\n","Epoch 89, Step 100, Loss: 0.0658\n","Epoch 89, Step 110, Loss: 0.0408\n","Epoch 89, Step 120, Loss: 0.0456\n","Epoch 89, Step 130, Loss: 0.0593\n","Epoch 89, Step 140, Loss: 0.0631\n","Epoch 89, Step 150, Loss: 0.0416\n","Average Loss in Epoch 89: 0.32665499404025905\n","-------------------------------------------------\n","Epoch 90, Step 10, Loss: 0.7518\n","Epoch 90, Step 20, Loss: 0.0718\n","Epoch 90, Step 30, Loss: 1.2875\n","Epoch 90, Step 40, Loss: 0.8988\n","Epoch 90, Step 50, Loss: 0.0604\n","Epoch 90, Step 60, Loss: 0.0279\n","Epoch 90, Step 70, Loss: 0.0247\n","Epoch 90, Step 80, Loss: 0.0501\n","Epoch 90, Step 90, Loss: 0.0576\n","Epoch 90, Step 100, Loss: 0.0480\n","Epoch 90, Step 110, Loss: 0.0515\n","Epoch 90, Step 120, Loss: 0.0589\n","Epoch 90, Step 130, Loss: 0.0290\n","Epoch 90, Step 140, Loss: 0.0592\n","Epoch 90, Step 150, Loss: 0.0430\n","Average Loss in Epoch 90: 0.3265337174020286\n","-------------------------------------------------\n","Epoch 91, Step 10, Loss: 0.7547\n","Epoch 91, Step 20, Loss: 0.0527\n","Epoch 91, Step 30, Loss: 0.0447\n","Epoch 91, Step 40, Loss: 0.0626\n","Epoch 91, Step 50, Loss: 0.0885\n","Epoch 91, Step 60, Loss: 0.5113\n","Epoch 91, Step 70, Loss: 0.0628\n","Epoch 91, Step 80, Loss: 0.0445\n","Epoch 91, Step 90, Loss: 2.6250\n","Epoch 91, Step 100, Loss: 0.0605\n","Epoch 91, Step 110, Loss: 0.0681\n","Epoch 91, Step 120, Loss: 1.8100\n","Epoch 91, Step 130, Loss: 0.0461\n","Epoch 91, Step 140, Loss: 0.0627\n","Epoch 91, Step 150, Loss: 0.0683\n","Average Loss in Epoch 91: 0.3263855020533193\n","-------------------------------------------------\n","Epoch 92, Step 10, Loss: 0.0506\n","Epoch 92, Step 20, Loss: 0.9060\n","Epoch 92, Step 30, Loss: 0.0566\n","Epoch 92, Step 40, Loss: 0.0607\n","Epoch 92, Step 50, Loss: 0.0452\n","Epoch 92, Step 60, Loss: 0.0754\n","Epoch 92, Step 70, Loss: 0.0443\n","Epoch 92, Step 80, Loss: 0.0564\n","Epoch 92, Step 90, Loss: 0.0496\n","Epoch 92, Step 100, Loss: 0.0542\n","Epoch 92, Step 110, Loss: 0.0683\n","Epoch 92, Step 120, Loss: 1.2435\n","Epoch 92, Step 130, Loss: 0.0418\n","Epoch 92, Step 140, Loss: 0.0581\n","Epoch 92, Step 150, Loss: 0.7293\n","Average Loss in Epoch 92: 0.3263637090113553\n","-------------------------------------------------\n","Epoch 93, Step 10, Loss: 0.0497\n","Epoch 93, Step 20, Loss: 0.0359\n","Epoch 93, Step 30, Loss: 0.0748\n","Epoch 93, Step 40, Loss: 3.1785\n","Epoch 93, Step 50, Loss: 0.0456\n","Epoch 93, Step 60, Loss: 0.0446\n","Epoch 93, Step 70, Loss: 0.0645\n","Epoch 93, Step 80, Loss: 0.0393\n","Epoch 93, Step 90, Loss: 1.5518\n","Epoch 93, Step 100, Loss: 2.9426\n","Epoch 93, Step 110, Loss: 0.0415\n","Epoch 93, Step 120, Loss: 0.0599\n","Epoch 93, Step 130, Loss: 2.8212\n","Epoch 93, Step 140, Loss: 1.3865\n","Epoch 93, Step 150, Loss: 2.3688\n","Average Loss in Epoch 93: 0.3262697438352138\n","-------------------------------------------------\n","Epoch 94, Step 10, Loss: 0.6688\n","Epoch 94, Step 20, Loss: 0.0482\n","Epoch 94, Step 30, Loss: 0.8985\n","Epoch 94, Step 40, Loss: 1.3519\n","Epoch 94, Step 50, Loss: 0.0519\n","Epoch 94, Step 60, Loss: 0.0674\n","Epoch 94, Step 70, Loss: 0.0638\n","Epoch 94, Step 80, Loss: 0.0667\n","Epoch 94, Step 90, Loss: 0.0857\n","Epoch 94, Step 100, Loss: 0.0495\n","Epoch 94, Step 110, Loss: 0.0590\n","Epoch 94, Step 120, Loss: 0.0374\n","Epoch 94, Step 130, Loss: 1.7197\n","Epoch 94, Step 140, Loss: 0.0681\n","Epoch 94, Step 150, Loss: 2.8582\n","Average Loss in Epoch 94: 0.32617165019395966\n","-------------------------------------------------\n","Epoch 95, Step 10, Loss: 1.5221\n","Epoch 95, Step 20, Loss: 0.0492\n","Epoch 95, Step 30, Loss: 0.0626\n","Epoch 95, Step 40, Loss: 0.0442\n","Epoch 95, Step 50, Loss: 0.5267\n","Epoch 95, Step 60, Loss: 0.0692\n","Epoch 95, Step 70, Loss: 0.0574\n","Epoch 95, Step 80, Loss: 0.0893\n","Epoch 95, Step 90, Loss: 0.0237\n","Epoch 95, Step 100, Loss: 1.5411\n","Epoch 95, Step 110, Loss: 0.9689\n","Epoch 95, Step 120, Loss: 0.0474\n","Epoch 95, Step 130, Loss: 3.0505\n","Epoch 95, Step 140, Loss: 0.0444\n","Epoch 95, Step 150, Loss: 0.0519\n","Average Loss in Epoch 95: 0.326114087746297\n","-------------------------------------------------\n","Epoch 96, Step 10, Loss: 0.0452\n","Epoch 96, Step 20, Loss: 0.0658\n","Epoch 96, Step 30, Loss: 0.0440\n","Epoch 96, Step 40, Loss: 0.0582\n","Epoch 96, Step 50, Loss: 0.0553\n","Epoch 96, Step 60, Loss: 0.0368\n","Epoch 96, Step 70, Loss: 0.0627\n","Epoch 96, Step 80, Loss: 0.0561\n","Epoch 96, Step 90, Loss: 0.0622\n","Epoch 96, Step 100, Loss: 0.0499\n","Epoch 96, Step 110, Loss: 0.0719\n","Epoch 96, Step 120, Loss: 0.0331\n","Epoch 96, Step 130, Loss: 0.0557\n","Epoch 96, Step 140, Loss: 1.4994\n","Epoch 96, Step 150, Loss: 0.0455\n","Average Loss in Epoch 96: 0.32590585002827943\n","-------------------------------------------------\n","Epoch 97, Step 10, Loss: 0.0482\n","Epoch 97, Step 20, Loss: 0.0770\n","Epoch 97, Step 30, Loss: 0.0614\n","Epoch 97, Step 40, Loss: 0.0394\n","Epoch 97, Step 50, Loss: 0.5110\n","Epoch 97, Step 60, Loss: 0.0690\n","Epoch 97, Step 70, Loss: 0.0672\n","Epoch 97, Step 80, Loss: 0.0467\n","Epoch 97, Step 90, Loss: 0.0495\n","Epoch 97, Step 100, Loss: 0.0458\n","Epoch 97, Step 110, Loss: 0.0557\n","Epoch 97, Step 120, Loss: 0.0385\n","Epoch 97, Step 130, Loss: 3.7568\n","Epoch 97, Step 140, Loss: 0.0887\n","Epoch 97, Step 150, Loss: 0.0492\n","Average Loss in Epoch 97: 0.32578213653474486\n","-------------------------------------------------\n","Epoch 98, Step 10, Loss: 0.0536\n","Epoch 98, Step 20, Loss: 0.0367\n","Epoch 98, Step 30, Loss: 0.0494\n","Epoch 98, Step 40, Loss: 0.0515\n","Epoch 98, Step 50, Loss: 0.9403\n","Epoch 98, Step 60, Loss: 0.0420\n","Epoch 98, Step 70, Loss: 0.0457\n","Epoch 98, Step 80, Loss: 0.6055\n","Epoch 98, Step 90, Loss: 1.8564\n","Epoch 98, Step 100, Loss: 0.0691\n","Epoch 98, Step 110, Loss: 0.0636\n","Epoch 98, Step 120, Loss: 0.7615\n","Epoch 98, Step 130, Loss: 0.0686\n","Epoch 98, Step 140, Loss: 0.0532\n","Epoch 98, Step 150, Loss: 0.0688\n","Average Loss in Epoch 98: 0.32569982240316253\n","-------------------------------------------------\n","Epoch 99, Step 10, Loss: 0.0351\n","Epoch 99, Step 20, Loss: 0.0785\n","Epoch 99, Step 30, Loss: 0.0421\n","Epoch 99, Step 40, Loss: 0.0432\n","Epoch 99, Step 50, Loss: 0.0275\n","Epoch 99, Step 60, Loss: 0.0565\n","Epoch 99, Step 70, Loss: 0.0507\n","Epoch 99, Step 80, Loss: 0.0591\n","Epoch 99, Step 90, Loss: 0.0407\n","Epoch 99, Step 100, Loss: 0.0449\n","Epoch 99, Step 110, Loss: 0.0856\n","Epoch 99, Step 120, Loss: 0.0482\n","Epoch 99, Step 130, Loss: 0.0581\n","Epoch 99, Step 140, Loss: 0.0604\n","Epoch 99, Step 150, Loss: 0.0500\n","Average Loss in Epoch 99: 0.32560778158546994\n","-------------------------------------------------\n","Epoch 100, Step 10, Loss: 0.0499\n","Epoch 100, Step 20, Loss: 0.0440\n","Epoch 100, Step 30, Loss: 0.0715\n","Epoch 100, Step 40, Loss: 0.0391\n","Epoch 100, Step 50, Loss: 0.0388\n","Epoch 100, Step 60, Loss: 0.0530\n","Epoch 100, Step 70, Loss: 0.0352\n","Epoch 100, Step 80, Loss: 0.8168\n","Epoch 100, Step 90, Loss: 0.0663\n","Epoch 100, Step 100, Loss: 0.0415\n","Epoch 100, Step 110, Loss: 1.3179\n","Epoch 100, Step 120, Loss: 0.0434\n","Epoch 100, Step 130, Loss: 0.0554\n","Epoch 100, Step 140, Loss: 0.0558\n","Epoch 100, Step 150, Loss: 0.5163\n","Average Loss in Epoch 100: 0.3255975831112584\n","-------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["1. For this project check-in your team must demonstrate the application of a neural network (NN)) to\n","your project for classification or prediction."],"metadata":{"id":"ruvgoBfUJB5s"}},{"cell_type":"markdown","source":["The data set is relatively simple, so we implement a 3-layer neural network."],"metadata":{"id":"UoVZ900ifT0a"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","input_size = x_train.shape[1]\n","output_size = 1\n","\n","# Experiment with different learning rates using the method covered during lecture (initializing to a small value then multiplying by a multiplier)\n","model = nn.Sequential(\n","    nn.Linear(input_size, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 32),\n","    nn.ReLU(),\n","    nn.Linear(32, output_size)\n",")\n","\n","initial_lr = 1e-5\n","optimizer = SGD(model.parameters(),lr = initial_lr)\n","\n","batch_size = 32\n","num_iterations = 100\n","final_lr = 10**1\n","lr_multiplier = (final_lr / initial_lr) ** (1 / num_iterations)\n","print(f\"Learning Rate Multiplier per Iteration: {lr_multiplier:.5f}\")\n","\n","train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).unsqueeze(1))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","lrs = []\n","losses = []\n","\n","train_iter = iter(train_loader)\n","\n","for iteration in range(1, num_iterations + 1):\n","    try:\n","        x_batch, y_batch = next(train_iter)\n","    except StopIteration:\n","        train_iter = iter(train_loader)\n","        x_batch, y_batch = next(train_iter)\n","\n","    pred = model(x_batch)\n","    loss = nn.MSELoss()(pred, y_batch)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    current_lr = optimizer.param_groups[0]['lr']\n","    lrs.append(current_lr)\n","    losses.append(loss.item())\n","\n","    if iteration % 10 == 0:\n","        print(f\"Iteration {iteration}, Loss: {loss.item():.4f}, Learning Rate: {current_lr:.5f}\")\n","\n","    new_lr = current_lr * lr_multiplier\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = new_lr\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Otiu2AafFNU","executionInfo":{"status":"ok","timestamp":1734005942531,"user_tz":480,"elapsed":309,"user":{"displayName":"Sonal Aggarwal","userId":"09254205623437368739"}},"outputId":"8cdad0fe-4fa7-44f6-84f4-496b0c575c3b","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning Rate Multiplier per Iteration: 1.14815\n","Iteration 10, Loss: 0.8786, Learning Rate: 0.00003\n","Iteration 20, Loss: 1.1266, Learning Rate: 0.00014\n","Iteration 30, Loss: 0.5637, Learning Rate: 0.00055\n","Iteration 40, Loss: 1.1198, Learning Rate: 0.00219\n","Iteration 50, Loss: 0.9239, Learning Rate: 0.00871\n","Iteration 60, Loss: 0.9143, Learning Rate: 0.03467\n","Iteration 70, Loss: 0.6175, Learning Rate: 0.13804\n","Iteration 80, Loss: 0.9570, Learning Rate: 0.54954\n","Iteration 90, Loss: 3192.7871, Learning Rate: 2.18776\n","Iteration 100, Loss: nan, Learning Rate: 8.70964\n"]}]},{"cell_type":"code","source":["# Plot Loss vs. Learning Rate\n","plt.figure(figsize=(10, 6))\n","plt.plot(lrs, losses)\n","plt.xscale('log')\n","plt.xlabel('Learning Rate')\n","plt.ylabel('Loss')\n","plt.title('Learning Rate Range Test')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":569},"id":"CJAL2538_mNZ","executionInfo":{"status":"ok","timestamp":1734005978617,"user_tz":480,"elapsed":891,"user":{"displayName":"Sonal Aggarwal","userId":"09254205623437368739"}},"outputId":"7136735e-4bbf-4e0b-df4b-4b85be10987c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","test_batch_size = 32\n","\n","\n","test_dataset = TensorDataset(\n","    torch.tensor(x_test, dtype=torch.float32),\n","    torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",")\n","test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n","\n","# Get the predictions of the model\n","model.eval()\n","predictions = []\n","actuals = []\n","\n","with torch.no_grad():\n","    for batch in test_loader:\n","        x_batch, y_batch = batch\n","        pred = model(x_batch)\n","        predictions.append(pred.cpu().numpy())\n","        actuals.append(y_batch.cpu().numpy())\n","\n","y_pred = np.vstack(predictions).flatten()\n","y_true = np.vstack(actuals).flatten()"],"metadata":{"id":"qHmkhkJQD5BG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, median_absolute_error\n","\n","# Unscale y_pred and y_true\n","y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n","y_true_original = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()\n","\n","# Calculate R-squared and MSE for the test set\n","rmse_test = np.sqrt(mean_squared_error(y_true_original, y_pred_original))\n","mae_test = mean_absolute_error(y_true_original, y_pred_original)\n","mad_test = median_absolute_error(y_true_original, y_pred_original)\n","r2_test = r2_score(y_true_original, y_pred_original)\n","corr_test = np.corrcoef(y_true_original, y_pred_original)[0, 1]  # Pearson correlation\n","\n","# Print the results for the test set\n","print(\"\\nTest Metrics:\")\n","print(f\"RMSE: {rmse_test}, MAE: {mae_test}, MAD: {mad_test}, Correlation: {corr_test}, R2: {r2_test}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGL2q25REXXV","executionInfo":{"status":"ok","timestamp":1734006135597,"user_tz":480,"elapsed":164,"user":{"displayName":"Sonal Aggarwal","userId":"09254205623437368739"}},"outputId":"23e4f4a1-64c7-4c78-fc97-36e1d547246b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test Metrics:\n","RMSE: 2.1947343349456787, MAE: 0.9101378917694092, MAD: 0.6199531555175781, Correlation: 0.830718669389917, R2: 0.6900213360786438\n"]}]},{"cell_type":"markdown","source":["2. Please explain the metrics you are using to assess the performance of your NN"],"metadata":{"id":"DWEvZvmZTYA-"}},{"cell_type":"markdown","source":["Our response variable is numerical variable, so the main metric we used is mean squared error. We saw the MSE loss while training the model, as we printed out the average loss for each epoch.\n","\n","MSE = sum ( (y_true - y_pred)^2)\n","\n","After the model was trained, we got the predictions from the model. Using the predictions and the true values, we then computed the MSE, rMSE, MAE, R2 score, and MAD. These are the typical metrics used for regression tasks."],"metadata":{"id":"XEYz1N-9g6yE"}},{"cell_type":"markdown","source":["3. Please also detail how you trained your NN including how you learned hyperparameters such as learning\n","rate.\n"],"metadata":{"id":"i1924FaPT4Ew"}},{"cell_type":"markdown","source":["First, we initially trained the model using a learning rate of 0.001. This did not work well for us, as the model was not converging. Then, we lowered the learning rate to 0.0001 to see if that would help by reducing oscillations, but the model still did not converge. Finally, we tried setting it to 0.002, which finally allowed it to converge. Trying these different learning rates and qualitatively looking at the metrics helped us to get a sense of the ideal learning rate, but a more robust method was to use the method we learned in lecture, which involved starting from 10^-5 to 10 in the end by multiplying the learning rate with a constant. After this, we made a plot of the loss vs. the learning rate to see which learning rate caused the loss to decrease before increasing sharply. Based on that, we settled on a learning rate of 0.13804, which had the best results in terms of getting the algorithm to converge at a relatively low loss (0.3).\n"],"metadata":{"id":"K-saC-DiZYu4"}},{"cell_type":"markdown","source":["4. Include code and explanation for the steps above in your notebook."],"metadata":{"id":"0w9UYVhyT4rz"}}]}