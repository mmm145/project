{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "# General Data Science libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Preprocessing and Linear Regression libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
        "\n",
        "\n",
        "# Libraries for the neural network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from  torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "\n",
        "# Pandas settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.options.mode.copy_on_write = True"
      ],
      "metadata": {
        "id": "qkmaNxKQtJgJ"
      },
      "id": "qkmaNxKQtJgJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1117978a",
      "metadata": {
        "id": "1117978a"
      },
      "outputs": [],
      "source": [
        "# Clean and preprocess the data by removing missing values and encoding categorical variables\n",
        "def preprocess_data():\n",
        "    # Loading the data\n",
        "    import kagglehub\n",
        "    # Get the path to the CSV file\n",
        "    path = kagglehub.dataset_download(\"lainguyn123/student-performance-factors\")\n",
        "    path = path + '/StudentPerformanceFactors.csv'\n",
        "    # Read the CSV file\n",
        "    student_performance_factors_original = pd.read_csv(path)\n",
        "    # Drop rows with missing values\n",
        "    miss_cat_cols = [\"Teacher_Quality\",\"Parental_Education_Level\",\"Distance_from_Home\"]\n",
        "    student_performance_factors_original = student_performance_factors_original.dropna(subset=miss_cat_cols)\n",
        "\n",
        "    # Encode categorical variables (convert them to numeric)\n",
        "    # Encode ordinal categorical variables using a custom mapping\n",
        "    Ordinal_cols = [\"Parental_Involvement\",\"Access_to_Resources\",\"Motivation_Level\",\"Family_Income\",\"Teacher_Quality\",\"Distance_from_Home\",\n",
        "                    \"Peer_Influence\", \"Parental_Education_Level\"]\n",
        "    ord_enc = OrdinalEncoder(categories=[\n",
        "        [\"Low\", \"Medium\", \"High\"],\n",
        "        [\"Low\", \"Medium\", \"High\"],\n",
        "        [\"Low\", \"Medium\", \"High\"],\n",
        "        [\"Low\", \"Medium\", \"High\"],\n",
        "        [\"Low\", \"Medium\", \"High\"],\n",
        "        [\"Near\", \"Moderate\", \"Far\"],\n",
        "        [\"Negative\", \"Neutral\", \"Positive\"],\n",
        "        [\"High School\", \"College\", \"Postgraduate\"]\n",
        "    ])\n",
        "\n",
        "    student_performance_factors_original[Ordinal_cols] = ord_enc.fit_transform(student_performance_factors_original[Ordinal_cols])\n",
        "\n",
        "    # Encode binary categorical variables\n",
        "    binary_cols = [\"Extracurricular_Activities\",\"Internet_Access\",\"Learning_Disabilities\",\"Gender\",\"School_Type\"]\n",
        "    for col in binary_cols:\n",
        "        le = LabelEncoder()\n",
        "        student_performance_factors_original[col] = le.fit_transform(student_performance_factors_original[col])\n",
        "\n",
        "    return student_performance_factors_original\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecdc50c8",
      "metadata": {
        "id": "ecdc50c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split_data(X, y, test_size=0.2, val_size=0.2):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Split into train+validation and test\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "    # Split train+validation into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size, random_state=42)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583e0916",
      "metadata": {
        "id": "583e0916"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_evaluate_linear_regression(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    # Train Linear Regression\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train, y_train)\n",
        "\n",
        "    # Get coefficients and intercept\n",
        "    coefficients = lr.coef_\n",
        "    intercept = lr.intercept_\n",
        "\n",
        "    # Get predictions\n",
        "    y_val_pred = lr.predict(X_val)\n",
        "    y_test_pred = lr.predict(X_test)\n",
        "\n",
        "    # Get metrics on validation set\n",
        "    rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "    mae_val = mean_absolute_error(y_val, y_val_pred)\n",
        "    mad_val = median_absolute_error(y_val, y_val_pred)\n",
        "    r2_val = r2_score(y_val, y_val_pred)\n",
        "    corr_val = np.corrcoef(y_val, y_val_pred)[0, 1]  # Pearson correlation\n",
        "\n",
        "    # Get metrics on test set\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "    mad_test = median_absolute_error(y_test, y_test_pred)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "    corr_test = np.corrcoef(y_test, y_test_pred)[0, 1]  # Pearson correlation\n",
        "\n",
        "    # Return the metrics\n",
        "    metrics = {\n",
        "        \"validation\": {\n",
        "            \"rMSE\": rmse_val,\n",
        "            \"MAE\": mae_val,\n",
        "            \"MAD\": mad_val,\n",
        "            \"Corr\": corr_val,\n",
        "            \"R2\": r2_val\n",
        "        },\n",
        "        \"test\": {\n",
        "            \"rMSE\": rmse_test,\n",
        "            \"MAE\": mae_test,\n",
        "            \"MAD\": mad_test,\n",
        "            \"Corr\": corr_test,\n",
        "            \"R2\": r2_test\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return (metrics, coefficients, intercept)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression workflow pipeline\n",
        "def linear_regression_workflow(features):\n",
        "  student_performance_factors_original = preprocess_data()\n",
        "  X_train, X_val, X_test, y_train, y_val, y_test = split_data(student_performance_factors_original[features],\n",
        "                                                    student_performance_factors_original[\"Exam_Score\"])\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train = scaler_x.fit_transform(X_train)\n",
        "  X_val = scaler_x.transform(X_val)\n",
        "  X_test = scaler_x.transform(X_test)\n",
        "  lr_output = train_and_evaluate_linear_regression(X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "  return lr_output"
      ],
      "metadata": {
        "id": "_IsmjV5g2kas"
      },
      "id": "_IsmjV5g2kas",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network workflow pipeline\n",
        "def neural_network_workflow(features, learning_rate, num_epochs):\n",
        "  student_performance_factors_original = preprocess_data()\n",
        "  if features == \"all\":\n",
        "    features = student_performance_factors_original.columns.drop(\"Exam_Score\")\n",
        "  X = student_performance_factors_original[features]\n",
        "  y = student_performance_factors_original[\"Exam_Score\"].values.reshape(-1, 1)\n",
        "  # Standardize the data for the neural network\n",
        "  scaler_x = StandardScaler()\n",
        "  scaler_y = StandardScaler()\n",
        "  X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
        "  X_train = scaler_x.fit_transform(X_train)\n",
        "  X_val = scaler_x.transform(X_val)\n",
        "  X_test = scaler_x.transform(X_test)\n",
        "  y_train = scaler_y.fit_transform(y_train).flatten()\n",
        "  y_val = scaler_y.transform(y_val).flatten()\n",
        "  y_test = scaler_y.transform(y_test).flatten()\n",
        "  # Toggle whether to find the learning rate or run the actual neural network algorithm by commenting out one of the below lines\n",
        "  # find_learning_rate(X_train, y_train)\n",
        "  nn_metrics = train_evaluate_neural_network(X_train, y_train, X_val, y_val, X_test, y_test, scaler_y, learning_rate, num_epochs)\n",
        "  return nn_metrics"
      ],
      "metadata": {
        "id": "cA5g_dJx6kX4"
      },
      "id": "cA5g_dJx6kX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features chosen from stepwise variable selection earlier\n",
        "features = ['Hours_Studied', 'Attendance', 'Parental_Involvement',\n",
        "       'Access_to_Resources', 'Previous_Scores', 'Tutoring_Sessions',\n",
        "       'Family_Income', 'Peer_Influence', 'Parental_Education_Level']\n",
        "\n",
        "# Train LR model using these features and get the metrics\n",
        "print(\"LR Metrics:\")\n",
        "lr_data = linear_regression_workflow(features)\n",
        "print(lr_data[0][\"test\"])\n",
        "coeffs = lr_data[1]\n",
        "for i in range(len(features)):\n",
        "    print(f\"{features[i]}: {coeffs[i]}\")\n",
        "print(f\"Intercept: {lr_data[2]}\")\n",
        "\n",
        "# Train the neural network on all features and get the metrics\n",
        "# (We tried training it on the same 9 features above but the\n",
        "# performance was worse, so it's better to use all the features)\n",
        "print(\"NN Metrics:\")\n",
        "print(neural_network_workflow(\"all\", learning_rate=0.13804, num_epochs=100)[\"test\"])\n",
        "# neural_network_workflow(\"all\", learning_rate=0.00055, num_epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiQXclJ092aD",
        "outputId": "52638c49-f7d0-4f35-b272-b41a040fede1"
      },
      "id": "oiQXclJ092aD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Metrics:\n",
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "{'rMSE': 2.1611559733001457, 'MAE': 0.8422462227654068, 'MAD': 0.5769952035716202, 'Corr': 0.8363517293644817, 'R2': 0.6994338463965144}\n",
            "Hours_Studied: 1.7701579861122858\n",
            "Attendance: 2.279441638910965\n",
            "Parental_Involvement: 0.6970962855836211\n",
            "Access_to_Resources: 0.7007387730024464\n",
            "Previous_Scores: 0.705267771248755\n",
            "Tutoring_Sessions: 0.6159218904227751\n",
            "Family_Income: 0.4138116742517593\n",
            "Peer_Influence: 0.3865435634220288\n",
            "Parental_Education_Level: 0.3928291806312596\n",
            "Intercept: 67.22788532222495\n",
            "NN Metrics:\n",
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "{'rMSE': 2.145385760251829, 'MAE': 0.6705396945199996, 'MAD': 0.3925056457519531, 'Corr': 0.840460670230925, 'R2': 0.7038043760492817}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "088a8227",
      "metadata": {
        "id": "088a8227"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_evaluate_neural_network(x_train, y_train, X_val, y_val, X_test, y_test, scaler_y, learning_rate, num_epochs):\n",
        "\n",
        "  # Train the model\n",
        "  input_size = x_train.shape[1]\n",
        "  output_size = 1\n",
        "\n",
        "  model = nn.Sequential(\n",
        "      nn.Linear(input_size, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, output_size)\n",
        "  )\n",
        "\n",
        "  # Batch normalization\n",
        "#   model = nn.Sequential(\n",
        "#     nn.Linear(input_size, 64),\n",
        "#     nn.ReLU(),\n",
        "#     nn.BatchNorm1d(64),\n",
        "#     nn.Linear(64, 32),\n",
        "#     nn.ReLU(),\n",
        "#     nn.BatchNorm1d(32),\n",
        "#     nn.Linear(32, 1)\n",
        "# )\n",
        "\n",
        "  # Dropout\n",
        "#   model = nn.Sequential(\n",
        "#     nn.Linear(input_size, 64),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.5),\n",
        "#     nn.Linear(64, 32),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.5),\n",
        "#     nn.Linear(32, 1)\n",
        "# )\n",
        "\n",
        "  # Try adding more layers to the network\n",
        "#   model = nn.Sequential(\n",
        "#     nn.Linear(input_size, 128),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(128, 64),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(64, 32),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(32, 1)\n",
        "# )\n",
        "\n",
        "\n",
        "  # Use SGD or Adam as the optimizer\n",
        "  optimizer = SGD(model.parameters(), learning_rate)\n",
        "  # Norm penalty\n",
        "  # optimizer = SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "  # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  epochs = num_epochs\n",
        "  b_size = 32\n",
        "  steps_per_epoch = len(x_train) // b_size\n",
        "\n",
        "  total_iters = len(x_train) // b_size * epochs\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      idx = np.random.permutation(len(x_train))\n",
        "      totalLoss = 0\n",
        "      for step in range(steps_per_epoch):\n",
        "          start_idx = step * b_size\n",
        "          end_idx = start_idx + b_size\n",
        "\n",
        "          x_batch = x_train[idx[start_idx:end_idx]]\n",
        "          idxx = idx[start_idx:end_idx]\n",
        "\n",
        "          y_batch = y_train[idx[start_idx:end_idx]]\n",
        "          x_batch_tensor = torch.tensor(x_batch, dtype=torch.float32)\n",
        "          y_batch_tensor = torch.tensor(y_batch, dtype=torch.float32).unsqueeze(1)  # Ensure shape (batch_size, 1)\n",
        "\n",
        "          pred = model(x_batch_tensor)\n",
        "          loss = nn.MSELoss()(pred, y_batch_tensor)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          totalLoss += loss.item()\n",
        "\n",
        "  # Evaluation on the test set\n",
        "  model.eval() # Set model to evaluation mode\n",
        "  x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "  with torch.no_grad():\n",
        "      y_test_pred = model(x_test_tensor).numpy()\n",
        "\n",
        "  # Unscale predictions and true values in order to have the metrics\n",
        "  # on the same scale to be able to compare linear regression and\n",
        "  # the neural network\n",
        "  y_test_pred_original = scaler_y.inverse_transform(y_test_pred).flatten()\n",
        "  y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "  # Calculate metrics on the test set\n",
        "  rmse_test = np.sqrt(mean_squared_error(y_test_original, y_test_pred_original))\n",
        "  mae_test = mean_absolute_error(y_test_original, y_test_pred_original)\n",
        "  mad_test = median_absolute_error(y_test_original, y_test_pred_original)\n",
        "  r2_test = r2_score(y_test_original, y_test_pred_original)\n",
        "  corr_test = np.corrcoef(y_test_original, y_test_pred_original)[0, 1]  # Pearson correlation\n",
        "\n",
        "  # Metrics\n",
        "  metrics = {\n",
        "      \"test\": {\n",
        "          \"rMSE\": rmse_test,\n",
        "          \"MAE\": mae_test,\n",
        "          \"MAD\": mad_test,\n",
        "          \"Corr\": corr_test,\n",
        "          \"R2\": r2_test,\n",
        "      }\n",
        "  }\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_learning_rate(x_train, y_train):\n",
        "  from torch.utils.data import TensorDataset, DataLoader\n",
        "  input_size = x_train.shape[1]\n",
        "  output_size = 1\n",
        "\n",
        "  # Experiment with different learning rates using the method covered during lecture\n",
        "  # (initializing to a small value then multiplying by a multiplier)\n",
        "\n",
        "  model = nn.Sequential(\n",
        "      nn.Linear(input_size, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, output_size)\n",
        "  )\n",
        "\n",
        "  # Batch normalization\n",
        "  #   model = nn.Sequential(\n",
        "  #     nn.Linear(input_size, 64),\n",
        "  #     nn.ReLU(),\n",
        "  #     nn.BatchNorm1d(64),\n",
        "  #     nn.Linear(64, 32),\n",
        "  #     nn.ReLU(),\n",
        "  #     nn.BatchNorm1d(32),\n",
        "  #     nn.Linear(32, 1)\n",
        "  # )\n",
        "\n",
        "  # Dropout\n",
        "  #   model = nn.Sequential(\n",
        "  #     nn.Linear(input_size, 64),\n",
        "  #     nn.ReLU(),\n",
        "  #     nn.Dropout(0.5),\n",
        "  #     nn.Linear(64, 32),\n",
        "  #     nn.ReLU(),\n",
        "  #     nn.Dropout(0.5),\n",
        "  #     nn.Linear(32, 1)\n",
        "  # )\n",
        "\n",
        "  # Try adding more layers to the network\n",
        "  #   model = nn.Sequential(\n",
        "  #     nn.Linear(input_size, 128),\n",
        "  #     nn.ReLU(),\n",
        "  #     nn.Linear(128, 64),\n",
        "  #     nn.ReLU(),\n",
        "  #     nn.Linear(64, 32),\n",
        "  #     nn.ReLU(),\n",
        "  #     nn.Linear(32, 1)\n",
        "  # )\n",
        "\n",
        "\n",
        "\n",
        "  # Set the initial learning rate\n",
        "  initial_lr = 1e-5\n",
        "\n",
        "  # Use SGD or Adam as the optimizer\n",
        "  optimizer = SGD(model.parameters(), lr = initial_lr)\n",
        "  # optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
        "  # Norm penalty\n",
        "  # optimizer = SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "  batch_size = 32\n",
        "  num_iterations = 100\n",
        "  final_lr = 10**1\n",
        "  lr_multiplier = (final_lr / initial_lr) ** (1 / num_iterations)\n",
        "  print(f\"Learning Rate Multiplier per Iteration: {lr_multiplier:.5f}\")\n",
        "\n",
        "  train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).unsqueeze(1))\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  lrs = []\n",
        "  losses = []\n",
        "\n",
        "  train_iter = iter(train_loader)\n",
        "\n",
        "  for iteration in range(1, num_iterations + 1):\n",
        "      try:\n",
        "          x_batch, y_batch = next(train_iter)\n",
        "      except StopIteration:\n",
        "          train_iter = iter(train_loader)\n",
        "          x_batch, y_batch = next(train_iter)\n",
        "\n",
        "      pred = model(x_batch)\n",
        "      loss = nn.MSELoss()(pred, y_batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      current_lr = optimizer.param_groups[0]['lr']\n",
        "      lrs.append(current_lr)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      if iteration % 10 == 0:\n",
        "          print(f\"Iteration {iteration}, Loss: {loss.item():.4f}, Learning Rate: {current_lr:.5f}\")\n",
        "\n",
        "      new_lr = current_lr * lr_multiplier\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = new_lr\n",
        "\n",
        "  lr_loss_df = pd.DataFrame({\"Learning Rate\": lrs, \"Loss\": losses})\n",
        "\n",
        "  # Plot learning rate vs. loss\n",
        "  fig = px.line(lr_loss_df, x=\"Learning Rate\", y=\"Loss\", title=\"Learning Rate Finder\")\n",
        "  fig.update_xaxes(type=\"log\", title=\"Learning Rate (log scale)\")\n",
        "  fig.update_yaxes(title=\"Loss\", range=[0, 10])  # Restrict y-axis to [0, 10]\n",
        "  fig.show()\n",
        "\n",
        "  return lrs, losses\n"
      ],
      "metadata": {
        "id": "Gag1emI9Ch7x"
      },
      "id": "Gag1emI9Ch7x",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}